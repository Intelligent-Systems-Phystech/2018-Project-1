\documentclass[12pt, twoside]{article}
\bibliographystyle{plain}
%\bibliography{Project1}
%\usepackage{jmlda}
\usepackage{hyperref}
\usepackage[english,russian]{babel}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{booktabs}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}

%\usepackage{afterpage}
%\usepackage{lipsum}% dummy text
\usepackage{lscape}

%\newcommand{\vect}[1]{\boldsymbol{#1}}
%\DeclareMathOperator*{\argmin}{argmin}


%\NOREVIEWERNOTES
\title
%    [Прогнозирование направления движения цены биржевых инструментов по новостному потоку ] % Краткое название; не нужно, если полное название влезает в~колонтитул
    {Бинарная классификация движения цен по новостному потоку}
\author
    {Ахияров~Валентин} % основной список авторов, выводимый в оглавление

%\email
%    {akhiarov.va@phystech.edu}

%\organization
%    {МФТИ (ГУ)}

\bigskip

\begin{document}
\maketitle
%\linenumbers

\subsection{Сравнение классификаторов на разных признаках}

\begin{table}[!htbp]
  \centering
  \begin{tabular}{clccccc}
  \toprule
  {} & {} &	{}	& {}	&	{} & \multicolumn{2}{c}{Average} \\
  \cmidrule(r){6-7}
  Classifier	&	Features	&	Data Set	&	F1 Score	&	AUC ROC	&	F1 Score	&	AUC ROC \\
  \midrule
  	&	Unigrams	&	1	&	0.7811	&	0.7181	&		&	 \\
  	&	Unigrams	&	2	&	0.8061	&	0.4973	&	\textbf{0.8093}	&	0.5565 \\
  	&	Unigrams	&	3	&	0.8408	&	0.4541	&		&	 \\
  	\cmidrule(r){2-7}
  	&	NMF 50	&	1	&	0.7397	&	0.6080	&		&	 \\
  	&	NMF 50	&	2	&	0.8087	&	0.5000	&	0.7647	&	0.5394 \\
  	&	NMF 50	&	3	&	0.7458	&	0.5102	&		&	 \\
  	\cmidrule(r){2-7}
  	&	NMF 100	&	1	&	0.7602	&	0.5841	&		&	 \\
  RF	&	NMF 100	&	2	&	0.8061	&	0.4973	&	0.7984	&	0.5487 \\
  	&	NMF 100	&	3	&	0.8288	&	0.5648	&		&	 \\
  	\cmidrule(r){2-7}
  	&	NMF 200	&	1	&	0.7720	&	0.7235	&		&	 \\
  	&	NMF 200	&	2	&	0.8087	&	0.5000	&	0.7996	&	\textbf{0.5838} \\
  	&	NMF 200	&	3	&	0.8180	&	0.5278	&		&	 \\
  	\cmidrule(r){2-7}
  	&	Ensemble	&	1	&	0.7907	&	0.7198	&		&	 \\
  	&	Ensemble	&	2	&	0.8018	&	0.5006	&	0.8045	&	0.5617 \\
  	&	Ensemble	&	3	&	0.821	&	0.4648	&		&	 \\
  \bottomrule
  \end{tabular}
  \caption{RandomForestClassifier on 3 data sets}
\end{table}

\begin{table}[!htbp]
  \centering
  \begin{tabular}{clccccc}
  \toprule
  {} & {} &	{}	& {}	&	{} & \multicolumn{2}{c}{Average} \\
  \cmidrule(r){6-7}
  Classifier	&	Features	&	Data Set	&	F1 Score	&	AUC ROC	&	F1 Score	&	AUC ROC \\
  \midrule
  	&	Unigrams	&	1	&	0.8371	&	0.7623	&		&	 \\
  	&	Unigrams	&	2	&	0.8035	&	0.4946	&	\textbf{0.835}	&	\textbf{0.5805} \\
  	&	Unigrams	&	3	&	0.8643	&	0.4846	&		&	 \\
  	\cmidrule(r){2-7}
  	&	NMF 50	&	1	&	0.8239	&	0.7508	&		&	 \\
  	&	NMF 50	&	2	&	0.8035	&	0.4946	&	0.8284	&	0.5716 \\
  	&	NMF 50	&	3	&	0.8577	&	0.4693	&		&	 \\
  	\cmidrule(r){2-7}
  	&	NMF 100	&	1	&	0.7989	&	0.7054	&		&	 \\
  XGB	&	NMF 100	&	2	&	0.8035	&	0.4946	&	0.8257	&	0.5586 \\
  	&	NMF 100	&	3	&	0.8747	&	0.4759	&		&	 \\
  	\cmidrule(r){2-7}
  	&	NMF 200	&	1	&	0.7923	&	0.6815	&		&	 \\
  	&	NMF 200	&	2	&	0.8061	&	0.4973	&	0.8221	&	0.5617 \\
  	&	NMF 200	&	3	&	0.8679	&	0.5063	&		&	 \\
  	\cmidrule(r){2-7}
  	&	Ensemble	&	1	&	0.8046	&	0.7314	&		&	 \\
  	&	Ensemble	&	2	&	0.8035	&	0.4946	&	0.8217	&	0.5680 \\
  	&	Ensemble	&	3	&	0.8571	&	0.4780	&		&	 \\
  \midrule
  	&	Unigrams	&	1	&	0.8217	&	0.6873	&		&	 \\
  	&	Unigrams	&	2	&	0.8087	&	0.5000	&	0.8464	&	\textbf{0.5624} \\
  	&	Unigrams	&	3	&	0.9087	&	0.5000	&		&	 \\
  	\cmidrule(r){2-7}
  	&	NMF 50	&	1	&	0.8235	&	0.6831	&		&	 \\
  	&	NMF 50	&	2	&	0.8087	&	0.5000	&	0.8470	&	0.5610 \\
  	&	NMF 50	&	3	&	0.9087	&	0.5000	&		&	 \\
  	\cmidrule(r){2-7}
  	&	NMF 100	&	1	&	0.8154	&	0.6724	&		&	 \\
  LR	&	NMF 100	&	2	&	0.8087	&	0.5000	&	0.8443	&	0.5575 \\
  	&	NMF 100	&	3	&	0.9087	&	0.5000	&		&	 \\
  	\cmidrule(r){2-7}
  	&	NMF 200	&	1	&	0.8244	&	0.6811	&		&	 \\
  	&	NMF 200	&	2	&	0.8087	&	0.5000	&	\textbf{0.8473}	&	0.5604 \\
  	&	NMF 200	&	3	&	0.9087	&	0.5000	&		&	 \\
  	\cmidrule(r){2-7}
  	&	Ensemble	&	1	&	0.8214	&	0.6782	&		&	 \\
  	&	Ensemble	&	2	&	0.8087	&	0.5000	&	0.8463	&	0.5594 \\
  	&	Ensemble	&	3	&	0.9087	&	0.5000	&		&	 \\
  \bottomrule
  \end{tabular}
  \caption{XGBClassifier \& LogisticRegression on 3 data sets}
\end{table}

\begin{table}[!htbp]
  \centering
  \begin{tabular}{clccccc}
  \toprule
  {} & {} &	{}	& {}	&	{} & \multicolumn{2}{c}{Average} \\
  \cmidrule(r){6-7}
  Classifier	&	Features	&	Data Set	&	F1 Score	&	AUC ROC	&	F1 Score	&	AUC ROC \\
  \midrule
  	&	Unigrams	&	1	&	0.7952	&	0.5957	&		&	 \\
  	&	Unigrams	&	2	&	0.8087	&	0.5000	&	0.8309	&	0.5406 \\
  	&	Unigrams	&	3	&	0.8889	&	0.5260	&		&	 \\
  	\cmidrule(r){2-7}
  	&	NMF 50	&	1	&	0.8049	&	0.6204	&		&	 \\
  	&	NMF 50	&	2	&	0.8087	&	0.5000	&	\textbf{0.8349}	&	\textbf{0.5495} \\
  	&	NMF 50	&	3	&	0.8912	&	0.5281	&		&	 \\
  	\cmidrule(r){2-7}
  	&	NMF 100	&	1	&	0.7933	&	0.5907	&		&	 \\
  LSVC	&	NMF 100	&	2	&	0.8087	&	0.5000	&	0.8310	&	0.5396 \\
  	&	NMF 100	&	3	&	0.8912	&	0.5281	&		&	 \\
  	\cmidrule(r){2-7}
  	&	NMF 200	&	1	&	0.7962	&	0.5936	&		&	 \\
  	&	NMF 200	&	2	&	0.8087	&	0.5000	&	0.8312	&	0.5399 \\
  	&	NMF 200	&	3	&	0.8889	&	0.5260	&		&	 \\
  	\cmidrule(r){2-7}
  	&	Ensemble	&	1	&	0.8029	&	0.6155	&		&	 \\
  	&	Ensemble	&	2	&	0.8087	&	0.5000	&	0.8343	&	0.5479 \\
  	&	Ensemble	&	3	&	0.8912	&	0.5281	&		&	 \\
  \bottomrule
  \end{tabular}
  \caption{LinearSVC on 3 data sets}
\end{table}

Будем обозначать модели Random Forest Classifier, XGB Classifier, Logistic Regression и Linear SVC как RF, XGB, LR и LSVC, соответственно.
Сравнив модели (RF, XGB, LR, LSVC) на разных признаках (Unigrams, NMF 50, NMF 100, NMF 200, Ensemble) выберем признаки, на которых модели давали лучший результат по F1 Score. Такими оказались: Unigrams (для моделей RF и XGB), NMF 50 (для модели LSVC) и NMF 200 (для модели LR). Далее будем оптимизировать модели с этими признаками (RF c Unigrams, XGB с Unigrams, LSVC с NMF 50 и LR с NMF 200) по гиперпараметрам.

\subsection{RandomForestClassifier с Unigrams}
\begin{landscape}
\begin{table}[!htbp]
  \centering
  \begin{tabular}{ccccccccc}
  \toprule
  {}	&	{}	&	{}	&	{}	&	{}	&	{}	&	{}	&	\multicolumn{2}{c}{Average} \\
  \cmidrule(r){8-9}
  max\_depth	&	min\_samples\_leaf	&	min\_samples\_split	&	n\_estimators	&	Data Set	&	F1 Score	&	AUC ROC	&	F1 Score	&	AUC ROC \\
  \midrule
  	&	&	&	&	1	&	0.8677	&	0.7710		&		&	 \\
  None	&	3	&	5	&	2000	&	2	&	0.8087	&	0.5000	&	0.8615	&	0.5961 \\
  	&	&	&	&	3	&	0.9080	&	0.5174	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.8663	&	0.7751	&		&	 \\
  None	&	3	&	2	&	2000	&	2	&	0.8087	&	0.5000	&	0.8611	&	0.5946 \\
  	&	&	&	&	3	&	0.9084	&	0.5087	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.8661	&	0.7640	&		&	 \\
  10	&	3	&	2	&	1000	&	2	&	0.8087	&	0.5000	&	0.8611	&	0.5909 \\
  	&	&	&	&	3	&	0.9084	&	0.5087	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.8647	&	0.7681	&		&	 \\
  None	&	3	&	2	&	1000	&	2	&	0.8087	&	0.5000	&	0.8605	&	0.5952 \\
  	&	&	&	&	3	&	0.9080	&	0.5174	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.8640	&	0.7702	&		&	 \\
  50	&	3	&	5	&	2000	&	2	&	0.8087	&	0.5000	&	0.8604	&	0.5930 \\
  	&	&	&	&	3	&	0.9084	&	0.5087	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.8639	&	0.7591	&		&	 \\
  10	&	3	&	5	&	2000	&	2	&	0.8087	&	0.5000	&	0.8603	&	0.5892 \\
  	&	&	&	&	3	&	0.9084	&	0.5087	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.8647	&	0.7681	&		&	 \\
  50	&	3	&	2	&	2000	&	2	&	0.8087	&	0.5000	&	0.8599	&	0.5915 \\
  	&	&	&	&	3	&	0.9062	&	0.5065	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.8624		&	0.7632	&		&	 \\
  50	&	3	&	2	&	1000	&	2	&	0.8087	&	0.5000	&	0.8597	&	0.5935 \\
  	&	&	&	&	3	&	0.9080	&	0.5174	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.8624	&	0.7632	&		&	 \\
  None	&	3	&	5	&	1000	&	2	&	0.8087	&	0.5000	&	0.8597	&	0.5935 \\
  	&	&	&	&	3	&	0.9080	&	0.5174	&		&	 \\
  \bottomrule
  \end{tabular}
  \caption{RandomForestClassifier with Unigrams}
\end{table}
\end{landscape}

\subsection{XGBClassifier с Unigrams}
\begin{landscape}
\begin{table}[!htbp]
  \centering
  \begin{tabular}{ccccccccc}
  \toprule
  {}	&	{}	&	{}	&	{}	&	{}	&	{}	&	{}	&	\multicolumn{2}{c}{Average} \\
  \cmidrule(r){8-9}
  gamma	&	learning\_rate	&	max\_depth	&	n\_estimators	&	Data Set	&	F1 Score	&	AUC ROC	&	F1 Score	&	AUC ROC \\
  \midrule
  	&	&	&	&	1	&	0.8291	&	0.7495	&		&	 \\
  0	&	0.1	&	3	&	50	&	2	&		&		&	0.8419	&	0.5824 \\
  	&	&	&	&	3	&	0.8880	&	0.4977	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.8391	&	0.7269	&		&	 \\
  0.2	&	0.1	&	9	&	50	&	2	&	0.8087	&	0.5000	&	0.8407	&	0.5705 \\
  	&	&	&	&	3	&	0.8742	&	0.4846 	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.8457	&	0.7417	&		&	 \\
  0.2	&	0.1	&	9	&	100	&	2	&	0.8087	&	0.5000	&	0.8398	&	0.5725 \\
  	&	&	&	&	3	&	0.8649	&	0.4759	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.8387	&	0.7380	&		&	 \\
  0.2	&	0.01	&	9	&	1000	&	2	&	0.8087	&	0.5000	&	0.8396	&	0.5764 \\
  	&	&	&	&	3	&	0.8714	&	0.4911	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.8415	&	0.7520	&		&	 \\
  0	&	0.1	&	9	&	500	&	2	&	0.8061	&	0.4973	&	0.8389	&	0.5794 \\
  	&	&	&	&	3	&	0.8690	&	0.4889	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.8343	&	0.7483	&		&	 \\
  0	&	0.1	&	9	&	1000	&	2	&	0.8061	&	0.4973	&	0.8388	&	0.5804 \\
  	&	&	&	&	3	&	0.8760	&	0.4955	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.8219	&	0.7256	&		&	 \\
  0.2	&	0.01	&	3	&	500	&	2	&	0.8087	&	0.5000	&	0.8388	&	0.5737 \\
  	&	&	&	&	3	&	0.8857	&	0.4955	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.8427	&	0.7388	&		&	 \\
  0.2	&	0.1	&	9	&	500	&	2	&	0.8087	&	0.5000	&	0.8387	&	0.5716 \\
  	&	&	&	&	3	&	0.8649	&	0.4759	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.8427	&	0.7388	&		&	 \\
  0.2	&	0.1	&	9	&	1000	&	2	&	0.8087	&	0.5000	&	0.8387	&	0.5716 \\
  	&	&	&	&	3	&	0.8649	&	0.4759	&		&	 \\
  \bottomrule
  \end{tabular}
  \caption{XGBClassifier with Unigrams}
\end{table}
\end{landscape}

\subsection{LinearSVC с NMF 50}
\begin{landscape}
\begin{table}[!htbp]
  \centering
  \begin{tabular}{ccccccccc}
  \toprule
  {}	&	{}	&	{}	&	{}	&	{}	&	{}	&	{}	&	\multicolumn{2}{c}{Average} \\
  \cmidrule(r){8-9}
  C	&	loss	&	max\_iter	&	multi\_class	&	Data Set	&	F1 Score	&	AUC ROC	&	F1 Score	&	AUC ROC \\
  \midrule
  	&	&	&	&	1	&	0.8152	&	0.6633	&		&	 \\
  0.1	&	squared\_hinge	&	1000	&	ovr	&	2	&	0.8087	&	0.5000	&	0.8442	&	0.5544 \\
  	&	&	&	&	3	&	0.9087	&	0.5000	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.8152	&	0.6633	&		&	 \\
  0.1	&	squared\_hinge	&	1500	&	ovr	&	2	&	0.8087	&	0.5000	&	0.8442	&	0.5544 \\
  	&	&	&	&	3	&	0.9087	&	0.5000	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.7852	&	0.5982	&		&	 \\
  0.1	&	hinge	&	1500	&	crammer\_singer	&	2	&	0.8087	&	0.5000	&	0.8342	&	0.5327 \\
  	&	&	&	&	3	&	0.9087	&	0.5000	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.7852	&	0.5982	&		&	 \\
  0.1	&	squared\_hinge	&	1500	&	crammer\_singer	&	2	&	0.8087	&	0.5000	&	0.8342	&	0.5327 \\
  	&	&	&	&	3	&	0.9087	&	0.5000	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.7852	&	0.5982	&		&	 \\
  0.1	&	squared\_hinge	&	1000	&	crammer\_singer	&	2	&	0.8087	&	0.5000	&	0.8342	&	0.5327 \\
  	&	&	&	&	3	&	0.9087	&	0.5000	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.7852	&	0.5982	&		&	 \\
  0.1	&	hinge	&	1000	&	crammer\_singer	&	2	&	0.8087	&	0.5000	&	0.8342	&	0.5327 \\
  	&	&	&	&	3	&	0.9087	&	0.5000	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.7841	&	0.6002	&		&	 \\
  1	&	hinge	&	1000	&	ovr	&	2	&	0.8087	&	0.5000	&	0.8338	&	0.5334 \\
  	&	&	&	&	3	&	0.9087	&	0.5000	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.7841	&	0.6002	&		&	 \\
  1	&	hinge	&	1500	&	ovr	&	2	&	0.8087	&	0.5000	&	0.8338	&	0.5334 \\
  	&	&	&	&	3	&	0.9087	&	0.5000	&		&	 \\
  	\cmidrule(r){5-9}
  	&	&	&	&	1	&	0.7786	&	0.5763	&		&	 \\
  0.1	&	hinge	&	1000	&	ovr	&	2	&	0.8087	&	0.5000	&	0.832	&	0.5254 \\
  	&	&	&	&	3	&	0.9087	&	0.5000	&		&	 \\
  \bottomrule
  \end{tabular}
  \caption{LinearSVC with NMF 50}
\end{table}
\end{landscape}

\subsection{LogisticRegression с NMF 200}
\begin{landscape}
\begin{table}[!htbp]
  \centering
  \begin{tabular}{cccccccc}
  \toprule
  {}	&	{}	&	{}	&	{}	&	{}	&	{}	&	\multicolumn{2}{c}{Average} \\
  \cmidrule(r){7-8}
  C	&	max\_iter	&	solver	&	Data Set	&	F1 Score	&	AUC ROC	&	F1 Score	&	AUC ROC \\
  \midrule
  	&	&	&	1	&	0.8235	&	0.6831	&		&	 \\
  1	&	150	&	liblinear	&	2	&	0.8087	&	0.5000	&	0.847	&	0.561 \\
  	&	&	&	3	&	0.9087	&	0.5000	&		&	 \\
  	\cmidrule(r){4-8}
  	&	&	&	1	&	0.8235	&	0.6831	&		&	 \\
 1	&	100	&	liblinear	&	2	&	0.8087	&	0.5000	&	0.847	&	0.561 \\
  	&	&	&	3	&	0.9087	&	0.5000	&		&	 \\
  	\cmidrule(r){4-8}
  	&	&	&	1	&	0.8173	&	0.6683	&		&	 \\
  1	&	150	&	newton-cg	&	2	&	0.8087	&	0.5000	&	0.8449	&	0.5561 \\
  	&	&	&	3	&	0.9087	&	0.5000	&		&	 \\
  	\cmidrule(r){4-8}
  	&	&	&	1	&	0.8173	&	0.6683	&		&	 \\
  1	&	100	&	lbfgs	&	2	&	0.8087	&	0.5000	&	0.8449	&	0.5561 \\
  	&	&	&	3	&	0.9087	&	0.5000	&		&	 \\
  	\cmidrule(r){4-8}
  	&	&	&	1	&	0.8173	&	0.6683	&		&	 \\
  1	&	100	&	newton-cg	&	2	&	0.8087	&	0.5000	&	0.8449	&	0.5561 \\
  	&	&	&	3	&	0.9087	&	0.5000	&		&	 \\
  	\cmidrule(r){4-8}
  	&	&	&	1	&	0.8173	&	0.6683	&		&	 \\
  1	&	150	&	lbfgs	&	2	&	0.8087	&	0.5000		&	0.8449	&	0.5561 \\
  	&	&	&	3	&	0.9087	&	0.5000		&		&	 \\
  	\cmidrule(r){4-8}
  	&	&	&	1	&	0.7960	&	0.6208	&		&	 \\
  0.1	&	100	&	liblinear	&	2	&	0.8087	&	0.5000	&	0.8378	&	0.5403 \\
  	&	&	&	3	&	0.9087	&	0.5000	&		&	 \\
  	\cmidrule(r){4-8}
  	&	&	&	1	&	0.7960	&	0.6208	&		&	 \\
  0.1	&	150	&	liblinear	&	2	&	0.8087	&	0.5000	&	0.8378	&	0.5403 \\
  	&	&	&	3	&	0.9087	&	0.5000	&		&	 \\
  	\cmidrule(r){4-8}
  	&	&	&	1	&	0.7904	&	0.5878	&		&	 \\
   0.1	&	100	&	lbfgs	&	2	&	0.8087	&	0.5000	&	0.8359	&	0.5293 \\
  	&	&	&	3	&	0.9087	&	0.5000	&		&	 \\
  \bottomrule
  \end{tabular}
  \caption{LogisticRegression with NMF 200}
\end{table}
\end{landscape}

\end{document}
