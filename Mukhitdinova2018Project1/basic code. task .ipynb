{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание Basic code.\n",
    "## По мотивам статьи: 2014 - On the Importance of Text Analysis for Stock Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasinkd/env/py36/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# load modules\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack, vstack\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 of 9 | ticker: AAPL\n",
      "iteration 2 of 9 | ticker: ADBE\n",
      "iteration 3 of 9 | ticker: AMZN\n",
      "iteration 4 of 9 | ticker: GOOG\n",
      "iteration 5 of 9 | ticker: HPQ\n",
      "iteration 6 of 9 | ticker: IBM\n",
      "iteration 7 of 9 | ticker: INTC\n",
      "iteration 8 of 9 | ticker: MSFT\n",
      "iteration 9 of 9 | ticker: NVDA\n",
      "finished. time elapsed: 87.99 sec\n"
     ]
    }
   ],
   "source": [
    "time_ = time.time()\n",
    "\n",
    "tickers = [\"AAPL\", \"ADBE\", \"AMZN\", \"GOOG\", \"HPQ\", \"IBM\", \"INTC\", \"MSFT\", \"NVDA\"] # tickers for {Apple,Adobe,Amazon,Google,HP,IBM,Intel,MicroSoft,NVidia}\n",
    "sp500_ticker = \"gspc\" # S&P500 ticker from paper\n",
    "\n",
    "reports_dataset = pd.DataFrame(columns=['ticker', 'date', 'time', 'text', 'movement', 'movement_normalized', 'label'])\n",
    "\n",
    "for iter_, ticker in enumerate(tickers):\n",
    "    ############################################\n",
    "    ################## PART 1 ##################\n",
    "    ############################################\n",
    "    \n",
    "    print('iteration %i of %i | ticker: %s' % (iter_+1, len(tickers), ticker))\n",
    "    \n",
    "    # load data\n",
    "    \n",
    "    # 1. stock quotes\n",
    "    price = pd.read_csv('data/price_history/'+ticker+'.csv')\n",
    "    price = price[price['Date'] > '2001-12-31']\n",
    "    price = price.sort_values('Date').reset_index(drop=True)\n",
    "    sp500 = pd.read_csv('data/price_history/'+sp500_ticker+'.csv')\n",
    "    sp500 = sp500[sp500['Date'] > '2001-12-31']\n",
    "    sp500 = sp500.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    # 2. 8K reports\n",
    "    with open('data/8K/'+ticker, 'r') as f:\n",
    "        f_lines = f.readlines()\n",
    "    raw_8k_reports = pd.Series(' '.join(f_lines).split('</DOCUMENT>')[:-1])\n",
    "\n",
    "    def transform_8k_report_to_dataframe(report):\n",
    "        # transform 8k report to pd.DataFrame row\n",
    "        #\n",
    "        # report: string\n",
    "        # result: pd.DataFrame\n",
    "\n",
    "        result = pd.DataFrame(columns=['ticker', 'date', 'time', 'text'], index=[0])\n",
    "\n",
    "        result['ticker'] = report.split('FILE:')[1].split('/')[0]\n",
    "\n",
    "        datetime_ = report.split('TIME:')[1].split('\\n')[0]\n",
    "        datetime_ = datetime.datetime.strptime(datetime_, '%Y%m%d%H%M%S')\n",
    "        result['date'] = str(datetime_.date())\n",
    "        result['time'] = str(datetime_.time())\n",
    "\n",
    "        text = report.split('ITEM:')[-1]\n",
    "        text = text.replace('QuickLinks', '').replace('Click here to rapidly navigate through this document', '')\n",
    "        text = ' '.join(text.split())\n",
    "        result['text'] = text\n",
    "\n",
    "        return result\n",
    "\n",
    "    reports = pd.DataFrame(columns=[])\n",
    "    for report in raw_8k_reports:\n",
    "        row = transform_8k_report_to_dataframe(report)\n",
    "        reports = pd.concat([reports, row], axis=0)\n",
    "    reports.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    del f_lines, raw_8k_reports, report\n",
    "    \n",
    "    \n",
    "    ############################################\n",
    "    ################## PART 2 ##################\n",
    "    ############################################\n",
    "    \n",
    "    # create binary markup {MOVE, STAY} for price data\n",
    "    # aggregate Up and DOWN labels to MOVE label\n",
    "\n",
    "    price_movement = pd.DataFrame(columns=['date', 'movement', 'movement_normalized', 'label'])\n",
    "\n",
    "    for i, j in price.iloc[:-1,:].iterrows():\n",
    "        row = pd.DataFrame(columns=['date', 'movement', 'movement_normalized', 'label'], index=[0])\n",
    "        row['date'] = j['Date']\n",
    "\n",
    "        column_next =  \"Open\"\n",
    "        column_prev =  \"Close\"\n",
    "        price_change = (price.loc[i+1, column_next] - price.loc[i, column_prev]) / price.loc[i, column_prev]\n",
    "        sp500_change = (sp500.loc[i+1, column_next] - sp500.loc[i, column_prev]) / sp500.loc[i, column_prev]\n",
    "        row['movement'] = price_change\n",
    "\n",
    "        price_change_normalized =  price_change - sp500_change\n",
    "        row['movement_normalized'] = price_change_normalized\n",
    "\n",
    "        if price_change_normalized >= 0.01: # value from paper\n",
    "            row['label'] = \"MOVE\" # price movement: UP\n",
    "        elif price_change_normalized <= -0.01: # value from paper\n",
    "            row['label'] =  \"MOVE\"# price movement: DOWN\n",
    "        else:\n",
    "            row['label'] =  \"STAY\"# price movement: STAY\n",
    "\n",
    "        price_movement = pd.concat([price_movement, row], axis=0)\n",
    "\n",
    "    price_movement.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    del price, sp500\n",
    "    \n",
    "    # merge stock quotes and text\n",
    "\n",
    "    reports = pd.merge(reports, price_movement, on='date', how='left')\n",
    "    reports.dropna(axis=0, inplace=True)\n",
    "\n",
    "    del price_movement\n",
    "    \n",
    "    # combine tickers\n",
    "    \n",
    "    reports_dataset = pd.concat([reports_dataset, reports], axis=0)\n",
    "    \n",
    "    del reports\n",
    "\n",
    "reports_dataset.reset_index(drop=True, inplace=True)\n",
    "print('finished. time elapsed: %.2f sec' % (time.time() - time_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reports_dataset.to_csv(\"ReportsDatasetStep2.csv\")\n",
    "# reports_dataset = pd.read_csv(\"ReportsDatasetStep2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "################## PART 3 ##################\n",
    "############################################\n",
    "\n",
    "# split data to train / validation / test\n",
    "\n",
    "# features\n",
    "train_start, train_end = '2001-12-31', '2008-12-31' # train period from paper\n",
    "x_train = reports_dataset[(train_start < reports_dataset['date']) & (reports_dataset['date'] <= train_end)]['text']\n",
    "\n",
    "val_start, val_end = train_end, '2010-12-31'# validation period from paper\n",
    "x_val = reports_dataset[(val_start < reports_dataset['date']) & (reports_dataset['date'] <= val_end)]['text']\n",
    "\n",
    "test_start, test_end =  val_end, '2012-12-31'# test period from paper\n",
    "x_test = reports_dataset[(test_start < reports_dataset['date']) & (reports_dataset['date'] <= test_end)]['text']\n",
    "\n",
    "# target variable\n",
    "# I added a comparison to \"MOVE\" in order to work with binary data explicitly\n",
    "y_train = reports_dataset[(train_start < reports_dataset['date']) & (reports_dataset['date'] <= train_end)]['label'] == \"MOVE\"\n",
    "y_val = reports_dataset[(val_start < reports_dataset['date']) & (reports_dataset['date'] <= val_end)]['label'] == \"MOVE\"\n",
    "y_test = reports_dataset[(test_start < reports_dataset['date']) & (reports_dataset['date'] <= test_end)]['label'] == \"MOVE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2411 features using unigrams\n",
      "nmf 50 features created\n",
      "nmf 100 features created\n",
      "nmf 200 features created\n",
      "finished. time elapsed: 114.03 sec\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "################## PART 4 ##################\n",
    "############################################\n",
    "\n",
    "time_ = time.time()\n",
    "\n",
    "# create features\n",
    "\n",
    "# 1. Unigrams\n",
    "# example: vectorizer_params = {}\n",
    "#          vectorizer = Vectorizer(**vectorizer_params)\n",
    "#          unigrams_train_features = vectorizer.fit_transform(x_train)\n",
    "# ...\n",
    "vectorizer_params = {\"ngram_range\": (1, 1), # we are using unigrams\n",
    "                     \"min_df\": 10} # according to authors\n",
    "\n",
    "# I do not remove stop-words since it was not mentioned in the original paper\n",
    "\n",
    "vectorizer = CountVectorizer(**vectorizer_params) \n",
    "unigrams_train_features = vectorizer.fit_transform(x_train)\n",
    "\n",
    "# I decided to try to implement feature selection using PMI\n",
    "# Found the implementation guide here: \n",
    "# https://stackoverflow.com/questions/46752650/information-gain-calculation-with-scikit-learn?rq=1\n",
    "\n",
    "feature_scores = mutual_info_classif(unigrams_train_features, y_train, \n",
    "                                     discrete_features=True,\n",
    "                                     random_state=RANDOM_STATE)\n",
    "\n",
    "vocab = [el for el, score in\n",
    "         zip(vectorizer.get_feature_names(), feature_scores) if\n",
    "         score > 0.01]\n",
    "\n",
    "# Using 0.01 as a threshold since we ends up with 2411 features\n",
    "# which is pretty close to 2319 features left in the original paper \n",
    "\n",
    "vectorizer_params['vocabulary'] = vocab\n",
    "\n",
    "vectorizer = CountVectorizer(**vectorizer_params) \n",
    "\n",
    "unigrams_train_features = vectorizer.fit_transform(x_train)\n",
    "unigrams_val_features = vectorizer.transform(x_val)\n",
    "unigrams_test_features = vectorizer.transform(x_test)\n",
    "\n",
    "print('Created {} features using unigrams'.format(unigrams_train_features.shape[1]))\n",
    "\n",
    "# 2. NMF vector for 50, 100, and 200 components\n",
    "# example: nmf_params = {}\n",
    "#          nmf = NMF(**nmf_params)\n",
    "#          nmf_train_features = nmf.fit_transform(unigrams_train_features)\n",
    "# ...\n",
    "\n",
    "nmf_params = {'n_components': 50,\n",
    "              'random_state': RANDOM_STATE,\n",
    "              'l1_ratio': 0.5,\n",
    "              'alpha': 0.1}\n",
    "# nmf_params were chosen based on\n",
    "# https://scikit-learn.org/0.18/auto_examples/applications/topics_extraction_with_nmf_lda.html\n",
    "\n",
    "nmf = NMF(**nmf_params)\n",
    "nmf_50_train_features = nmf.fit_transform(unigrams_train_features)\n",
    "nmf_50_val_features = nmf.transform(unigrams_val_features)\n",
    "nmf_50_test_features = nmf.transform(unigrams_test_features)\n",
    "\n",
    "print('nmf 50 features created')\n",
    "\n",
    "nmf_params['n_components'] = 100\n",
    "nmf = NMF(**nmf_params)\n",
    "nmf_100_train_features = nmf.fit_transform(unigrams_train_features)\n",
    "nmf_100_val_features = nmf.transform(unigrams_val_features)\n",
    "nmf_100_test_features = nmf.transform(unigrams_test_features)\n",
    "\n",
    "print('nmf 100 features created')\n",
    "\n",
    "nmf_params['n_components'] = 200\n",
    "nmf = NMF(**nmf_params)\n",
    "nmf_200_train_features = nmf.fit_transform(unigrams_train_features)\n",
    "nmf_200_val_features = nmf.transform(unigrams_val_features)\n",
    "nmf_200_test_features = nmf.transform(unigrams_test_features)\n",
    "\n",
    "print('nmf 200 features created')\n",
    "print('finished. time elapsed: %.2f sec' % (time.time() - time_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "################## PART 5 ##################\n",
    "############################################\n",
    "\n",
    "# combine all features to one feature space\n",
    "\n",
    "# 1. NMF 50 features\n",
    "nmf_50_x_train = hstack([unigrams_train_features, \n",
    "                         nmf_50_train_features])\n",
    "\n",
    "nmf_50_x_val = hstack([unigrams_val_features,   \n",
    "                       nmf_50_val_features])\n",
    "\n",
    "nmf_50_x_test = hstack([unigrams_test_features,  \n",
    "                        nmf_50_test_features])\n",
    "\n",
    "# 2. NMF 100 features\n",
    "nmf_100_x_train = hstack([unigrams_train_features, \n",
    "                         nmf_100_train_features])\n",
    "\n",
    "nmf_100_x_val = hstack([unigrams_val_features,   \n",
    "                       nmf_100_val_features])\n",
    "\n",
    "nmf_100_x_test = hstack([unigrams_test_features,  \n",
    "                        nmf_100_test_features])\n",
    "\n",
    "\n",
    "# 3. NMF 200 features\n",
    "nmf_200_x_train = hstack([unigrams_train_features, \n",
    "                         nmf_200_train_features])\n",
    "\n",
    "nmf_200_x_val = hstack([unigrams_val_features,   \n",
    "                       nmf_200_val_features])\n",
    "\n",
    "nmf_200_x_test = hstack([unigrams_test_features,  \n",
    "                        nmf_200_test_features])\n",
    "\n",
    "# 4. Ensemble features\n",
    "ensemble_x_train = hstack([unigrams_train_features, \n",
    "                           nmf_50_train_features, \n",
    "                           nmf_100_train_features, \n",
    "                           nmf_200_train_features])\n",
    "\n",
    "ensemble_x_val = hstack([unigrams_val_features, \n",
    "                         nmf_50_val_features, \n",
    "                         nmf_100_val_features, \n",
    "                         nmf_200_val_features])\n",
    "\n",
    "ensemble_x_test = hstack([unigrams_test_features, \n",
    "                          nmf_50_test_features, \n",
    "                          nmf_100_test_features, \n",
    "                          nmf_200_test_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_unigrams_model trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_nmf_50_model trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_nmf_100_model trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_nmf_200_model trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_ensemble_model trained\n",
      "finished. time elapsed: 30.28 sec\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "################## PART 6 ##################\n",
    "############################################\n",
    "\n",
    "time_ = time.time()\n",
    "\n",
    "# set basic classifier parameters, fit classifiers. \n",
    "# Hint: initialize new basic classifier before new training process\n",
    "\n",
    "# example: basic_classifier_params = {}\n",
    "#          basic_classifier = BasicClassifier(**basic_classifier_params)\n",
    "\n",
    "\n",
    "# 1. Unigrams\n",
    "# example: rf_unigrams_model = ...\n",
    "# ...\n",
    "rf_params = {\"n_estimators\": 2000, # 2000 trees according to the paper\n",
    "             \"n_jobs\": -1,\n",
    "             \"random_state\": RANDOM_STATE}\n",
    "rf_unigrams_model = RandomForestClassifier(**rf_params)\n",
    "rf_unigrams_model.fit(unigrams_train_features, y_train)\n",
    "\n",
    "print('rf_unigrams_model trained')\n",
    "\n",
    "# 2. NMF 50\n",
    "\n",
    "rf_nmf_50_model = RandomForestClassifier(**rf_params)\n",
    "rf_nmf_50_model.fit(nmf_50_x_train, y_train)\n",
    "\n",
    "print('rf_nmf_50_model trained')\n",
    "\n",
    "# 3. NMF 100\n",
    "rf_nmf_100_model = RandomForestClassifier(**rf_params)\n",
    "rf_nmf_100_model.fit(nmf_100_x_train, y_train)\n",
    "\n",
    "print('rf_nmf_100_model trained')\n",
    "\n",
    "# 4. NMF 200\n",
    "rf_nmf_200_model = RandomForestClassifier(**rf_params)\n",
    "rf_nmf_200_model.fit(nmf_200_x_train, y_train)\n",
    "\n",
    "print('rf_nmf_200_model trained')\n",
    "\n",
    "# 5. Ensemble\n",
    "rf_ensemble_model = RandomForestClassifier(**rf_params)\n",
    "rf_ensemble_model.fit(ensemble_x_train, y_train)\n",
    "\n",
    "print('rf_ensemble_model trained')\n",
    "print('finished. time elapsed: %.2f sec' % (time.time() - time_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>n_features</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rf_unigrams_model</td>\n",
       "      <td>2411</td>\n",
       "      <td>0.760163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf_nmf_50_model</td>\n",
       "      <td>2461</td>\n",
       "      <td>0.764228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rf_nmf_100_model</td>\n",
       "      <td>2511</td>\n",
       "      <td>0.768293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf_nmf_200_model</td>\n",
       "      <td>2611</td>\n",
       "      <td>0.764228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf_ensemble_model</td>\n",
       "      <td>2761</td>\n",
       "      <td>0.760163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model  n_features  accuracy\n",
       "0  rf_unigrams_model        2411  0.760163\n",
       "1    rf_nmf_50_model        2461  0.764228\n",
       "2   rf_nmf_100_model        2511  0.768293\n",
       "3   rf_nmf_200_model        2611  0.764228\n",
       "4  rf_ensemble_model        2761  0.760163"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################\n",
    "################## PART 7 ##################\n",
    "############################################\n",
    "\n",
    "# calculate quality measures\n",
    "\n",
    "# 1. Unigrams\n",
    "# example: rf_unigrams_quality = Quality(y_true, y_pred)\n",
    "\n",
    "y_pred = rf_unigrams_model.predict(unigrams_test_features)\n",
    "rf_unigrams_quality = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# 2. NMF 50\n",
    "y_pred = rf_nmf_50_model.predict(nmf_50_x_test)\n",
    "rf_nmf_50_quality = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# 3. NMF 100\n",
    "y_pred = rf_nmf_100_model.predict(nmf_100_x_test)\n",
    "rf_nmf_100_quality = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# 4. NMF 200\n",
    "y_pred = rf_nmf_200_model.predict(nmf_200_x_test)\n",
    "rf_nmf_200_quality = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# 5. Ensemble\n",
    "y_pred = rf_ensemble_model.predict(ensemble_x_test)\n",
    "rf_ensemble_quality = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# save results\n",
    "\n",
    "results = pd.DataFrame({'model': ['rf_unigrams_model', 'rf_nmf_50_model', \n",
    "                                  'rf_nmf_100_model', 'rf_nmf_200_model', 'rf_ensemble_model'], \n",
    "                        'n_features': [rf_unigrams_model.n_features_, rf_nmf_50_model.n_features_, \n",
    "                                       rf_nmf_100_model.n_features_, rf_nmf_200_model.n_features_, \n",
    "                                       rf_ensemble_model.n_features_], \n",
    "                        'accuracy': [rf_unigrams_quality, rf_nmf_50_quality, \n",
    "                                     rf_nmf_100_quality, rf_nmf_200_quality, rf_ensemble_quality]}, \n",
    "                       columns=['model', 'n_features', 'accuracy'])\n",
    "results.to_csv('data/results.csv', index=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I have less features since I'm using PMI for feature selection as it was mentioned in the original paper. I've decided to leave calculations with less features since it works faster and shows a better result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect where our best model makes errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[131,  27],\n",
       "       [ 30,  58]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rf_nmf_100_model.predict(nmf_100_x_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be pretty hard to analize false pozitive errors. Let's try to take a closer look to the example of false positive error (model believes that the piece of news will hane an effect on the share price)\n",
    "\n",
    "*Just a reminder: I set \"MOVE\" answer as True and \"STAY\" as False*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Submission of Matters to a Vote of Security Holders Check the appropriate box below if the Form 8-K filing is intended to simultaneously satisfy the filing obligation of the registrant under any of the following provisions: Item 5.07 Submission of Matters to a Vote of Security Holders The Annual Meeting of Shareholders (the \"Annual Meeting\") of Apple Inc. (the \"Company\") was held on February 23, 2011. At the Annual Meeting, the shareholders voted on the following six proposals and cast their votes as described below. Proposal 1 The individuals listed below were elected at the Annual Meeting to serve a one-year term on the Company\\'s Board of Directors (the \"Board\"). Proposal 2 Proposal 2 was a management proposal to ratify the appointment of Ernst & Young LLP as the Company\\'s independent registered public accounting firm for fiscal year 2011, as described in the proxy materials. This proposal was approved. Proposal 3 Proposal 3 was a management proposal to hold an advisory vote on executive compensation, as described in the proxy materials. This proposal was approved. Proposal 4 Proposal 4 was a management proposal to hold an advisory vote on the frequency of the advisory vote on executive compensation, as described in the proxy materials. \"1 Year\" was approved. Based on these results, and consistent with the Company\\'s recommendation, the Board has determined that the Company will hold an advisory vote on executive compensation every year. Proposal 5 Proposal 5 was a shareholder proposal entitled \"Amend the Company\\'s Corporate Governance Guidelines to Adopt and Disclose a Written CEO Succession Planning Policy,\" as described in the proxy materials. This proposal was not approved. Proposal 6 Proposal 6 was a shareholder proposal entitled \"Adopt a Majority Voting Standard for Director Elections,\" as described in the proxy materials. This proposal was approved. Pursuant to the requirements of the Securities Exchange Act of 1934, the Registrant has duly caused this report to be signed on its behalf by the undersigned, thereunto duly authorized.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positive = ((y_test == False) & (y_pred == True))\n",
    "x_test[false_positive.index[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We may see here some positive words like \"satisfy\", \"proposals\", \"approved\", \"recommendation\", \"consistent\". Yet, we, as a human beings may understand that it is not so positive information: this is all mainly about some company governance tasks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section I'm going to check the performance of XGBClassifier, LinearSVC and LogisticRegression on the dataset created in the previous section using f1-score and roc-auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use not tuned classifiers in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams_models trained\n",
      "nmf_50_models trained\n",
      "nmf_100_models trained\n",
      "nmf_200_models trained\n",
      "ensemble_models trained\n",
      "finished. time elapsed: 10.63 sec\n"
     ]
    }
   ],
   "source": [
    "time_ = time.time()\n",
    "\n",
    "params = {\"random_state\": RANDOM_STATE}\n",
    "\n",
    "def create_and_fit_models(X, y, params):\n",
    "    return {\"xgb\": XGBClassifier(n_jobs=-1, **params).fit(X, y),\n",
    "            \"logit\": LogisticRegression(**params).fit(X, y),\n",
    "            \"svm\": LinearSVC(**params).fit(X, y),\n",
    "            \"rf\": RandomForestClassifier(**params).fit(X, y)}\n",
    "\n",
    "# 1. Unigrams\n",
    "\n",
    "\n",
    "unigrams_models = create_and_fit_models(unigrams_train_features, y_train, params)\n",
    "\n",
    "print('unigrams_models trained')\n",
    "\n",
    "# 2. NMF 50\n",
    "\n",
    "nmf_50_models = create_and_fit_models(nmf_50_x_train, y_train, params)\n",
    "\n",
    "print('nmf_50_models trained')\n",
    "\n",
    "# 3. NMF 100\n",
    "nmf_100_models = create_and_fit_models(nmf_100_x_train, y_train, params)\n",
    "\n",
    "print('nmf_100_models trained')\n",
    "\n",
    "# 4. NMF 200\n",
    "nmf_200_models = create_and_fit_models(nmf_200_x_train, y_train, params)\n",
    "\n",
    "print('nmf_200_models trained')\n",
    "\n",
    "# 5. Ensemble\n",
    "ensemble_models = create_and_fit_models(ensemble_x_train, y_train, params)\n",
    "\n",
    "print('ensemble_models trained')\n",
    "print('finished. time elapsed: %.2f sec' % (time.time() - time_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasinkd/env/py36/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/vasinkd/env/py36/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/vasinkd/env/py36/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/vasinkd/env/py36/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/vasinkd/env/py36/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "def get_quality_scores(features_name, models, X_test, y_test):\n",
    "    res = []\n",
    "    for model_name, model in models.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred)\n",
    "        res.append({\"model\": model_name,\n",
    "                    \"features\": features_name,\n",
    "                    \"f1 score\": f1,\n",
    "                    \"roc_auc_score\": roc_auc})\n",
    "    return res\n",
    "    \n",
    "res = []\n",
    "# 1. Unigrams\n",
    "# example: rf_unigrams_quality = Quality(y_true, y_pred)\n",
    "\n",
    "res.extend(get_quality_scores(\"unigrams\", unigrams_models,\n",
    "                              unigrams_test_features, y_test))\n",
    "\n",
    "# 2. NMF 50\n",
    "res.extend(get_quality_scores(\"nmf_50\", nmf_50_models,\n",
    "                              nmf_50_x_test, y_test))\n",
    "\n",
    "# 3. NMF 100\n",
    "res.extend(get_quality_scores(\"nmf_100\", nmf_100_models,\n",
    "                               nmf_100_x_test, y_test))\n",
    "\n",
    "# 4. NMF 200\n",
    "res.extend(get_quality_scores(\"nmf_200\", nmf_200_models,\n",
    "                               nmf_200_x_test, y_test))\n",
    "\n",
    "# 5. Ensemble\n",
    "res.extend(get_quality_scores(\"ensemble\", ensemble_models,\n",
    "                               ensemble_x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res, columns=[\"model\", \"features\", \"f1 score\", \"roc_auc_score\"])\n",
    "df.sort_values([\"model\", \"f1 score\"], ascending=False, inplace=True)\n",
    "\n",
    "df.to_csv('data/results2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>features</th>\n",
       "      <th>f1 score</th>\n",
       "      <th>roc_auc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xgb</td>\n",
       "      <td>unigrams</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.740291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xgb</td>\n",
       "      <td>nmf_200</td>\n",
       "      <td>0.651429</td>\n",
       "      <td>0.728927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>xgb</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>0.643275</td>\n",
       "      <td>0.723892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xgb</td>\n",
       "      <td>nmf_50</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>0.718211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xgb</td>\n",
       "      <td>nmf_100</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>0.718211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>svm</td>\n",
       "      <td>nmf_50</td>\n",
       "      <td>0.560440</td>\n",
       "      <td>0.653697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>svm</td>\n",
       "      <td>nmf_200</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.643556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svm</td>\n",
       "      <td>unigrams</td>\n",
       "      <td>0.544503</td>\n",
       "      <td>0.634062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>svm</td>\n",
       "      <td>nmf_100</td>\n",
       "      <td>0.544503</td>\n",
       "      <td>0.634062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>svm</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>0.647440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rf</td>\n",
       "      <td>nmf_50</td>\n",
       "      <td>0.685393</td>\n",
       "      <td>0.754819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf</td>\n",
       "      <td>unigrams</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.740291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rf</td>\n",
       "      <td>nmf_200</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.742880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rf</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>0.639053</td>\n",
       "      <td>0.721375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rf</td>\n",
       "      <td>nmf_100</td>\n",
       "      <td>0.614525</td>\n",
       "      <td>0.698576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit</td>\n",
       "      <td>unigrams</td>\n",
       "      <td>0.527473</td>\n",
       "      <td>0.627158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit</td>\n",
       "      <td>nmf_50</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.620829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>logit</td>\n",
       "      <td>nmf_100</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.620829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logit</td>\n",
       "      <td>nmf_200</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.620829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>logit</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.614499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model  features  f1 score  roc_auc_score\n",
       "0   xgb    unigrams  0.666667  0.740291     \n",
       "12  xgb    nmf_200   0.651429  0.728927     \n",
       "16  xgb    ensemble  0.643275  0.723892     \n",
       "4   xgb    nmf_50    0.635294  0.718211     \n",
       "8   xgb    nmf_100   0.635294  0.718211     \n",
       "6   svm    nmf_50    0.560440  0.653697     \n",
       "14  svm    nmf_200   0.553191  0.643556     \n",
       "2   svm    unigrams  0.544503  0.634062     \n",
       "10  svm    nmf_100   0.544503  0.634062     \n",
       "18  svm    ensemble  0.541176  0.647440     \n",
       "7   rf     nmf_50    0.685393  0.754819     \n",
       "3   rf     unigrams  0.666667  0.740291     \n",
       "15  rf     nmf_200   0.666667  0.742880     \n",
       "19  rf     ensemble  0.639053  0.721375     \n",
       "11  rf     nmf_100   0.614525  0.698576     \n",
       "1   logit  unigrams  0.527473  0.627158     \n",
       "5   logit  nmf_50    0.521739  0.620829     \n",
       "9   logit  nmf_100   0.521739  0.620829     \n",
       "13  logit  nmf_200   0.521739  0.620829     \n",
       "17  logit  ensemble  0.516129  0.614499     "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may see that not tuned RF and XGB models outperforms logit and svm models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we may use our validation dataset to tune parameters of our models!\n",
    "\n",
    "We are going to tune XGB and logit models using unigrams as features; RF and SVM models - using nmf_50 features.\n",
    "\n",
    "We will use f1 score and roc-auc score to compare our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from tqdm import tqdm_notebook\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_dataset(dataset):\n",
    "    \"\"\"\n",
    "    we want to tune parameters using validation set\n",
    "    GridSearchCV with default cv param wouldn't help us with that\n",
    "    We need to use PredefinedSplit as mentioned in\n",
    "    https://stackoverflow.com/questions/43764999/python-machine-learning-perform-a-grid-search-on-custom-validation-set/43766334#43766334\n",
    "    \"\"\"\n",
    "    my_test_fold = []\n",
    "\n",
    "    for i in range(dataset[\"X_train\"].shape[0]):\n",
    "        my_test_fold.append(-1)\n",
    "\n",
    "    for i in range(dataset[\"X_val\"].shape[0]):\n",
    "        my_test_fold.append(0)\n",
    "    \n",
    "    dataset['X_united'] = sparse.vstack([dataset[\"X_train\"], dataset[\"X_val\"]])\n",
    "    dataset['y_united'] = np.concatenate([y_train.values, y_val.values])\n",
    "    dataset['cv'] = PredefinedSplit(test_fold=my_test_fold)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unigrams_dataset = prep_dataset(\n",
    "    {\"X_train\": unigrams_train_features,\n",
    "     \"y_train\": y_train,\n",
    "     \"X_val\": unigrams_val_features,\n",
    "     \"y_val\": y_val,\n",
    "     \"X_test\": unigrams_test_features,\n",
    "     \"y_test\": y_test})\n",
    "\n",
    "nmf_50_dataset = prep_dataset(\n",
    "    {\"X_train\": nmf_50_x_train,\n",
    "     \"y_train\": y_train,\n",
    "     \"X_val\": nmf_50_x_val,\n",
    "     \"y_val\": y_val,\n",
    "     \"X_test\": nmf_50_x_test,\n",
    "     \"y_test\": y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_to_dataset = {\n",
    "    \"xgb\": unigrams_dataset,\n",
    "    \"logit\": unigrams_dataset,\n",
    "    \"svm\": nmf_50_dataset,\n",
    "    \"rf\": nmf_50_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = {\"xgb\": XGBClassifier(random_state=RANDOM_STATE),\n",
    "         \"logit\": LogisticRegression(random_state=RANDOM_STATE),\n",
    "         \"svm\": LinearSVC(random_state=RANDOM_STATE),\n",
    "         \"rf\": RandomForestClassifier(random_state=RANDOM_STATE)}\n",
    "\n",
    "all_model_params = {\n",
    "    \"xgb\": {\"gamma\": [0., 0.3, 0.5],\n",
    "            \"max_depth\": [3, 6, 10],\n",
    "            \"n_estimators\": [20, 200]},\n",
    "    \"logit\": {\"penalty\": [\"l1\", \"l2\"],\n",
    "              \"class_weight\": [None, \"balanced\"],\n",
    "              \"C\": [0.1, 1, 10, 100]},\n",
    "    \"svm\": {\"class_weight\": [None, \"balanced\"],\n",
    "            \"C\": [0.1, 1, 10, 100]},\n",
    "    \"rf\": {\"class_weight\": [None], # excluded \"balanced\" since it always loses to None\n",
    "           \"n_estimators\": [10, 20, 200, 2000]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7194d187fb41c1b7b4840b827ab37b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic',\n",
       "       random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'gamma': [0.0, 0.3, 0.5], 'max_depth': [3, 6, 10], 'n_estimators': [20, 200]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(roc_auc_score), verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'class_weight': [None, 'balanced'], 'C': [0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(roc_auc_score), verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
       "     verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'class_weight': [None, 'balanced'], 'C': [0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(roc_auc_score), verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'class_weight': [None], 'n_estimators': [10, 20, 200, 2000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(roc_auc_score), verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic',\n",
       "       random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'gamma': [0.0, 0.3, 0.5], 'max_depth': [3, 6, 10], 'n_estimators': [20, 200]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(f1_score), verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'class_weight': [None, 'balanced'], 'C': [0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(f1_score), verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
       "     verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'class_weight': [None, 'balanced'], 'C': [0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(f1_score), verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'class_weight': [None], 'n_estimators': [10, 20, 200, 2000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(f1_score), verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "with tqdm_notebook(total=2*len(models)) as f:\n",
    "    for scorer_func, scorer_name in [(roc_auc_score, \"roc-auc\"), (f1_score, \"f1\")]:\n",
    "        for model_name, model in models.items():\n",
    "            model_dataset = model_to_dataset[model_name]\n",
    "            model_params = all_model_params[model_name]\n",
    "            gs = GridSearchCV(model, param_grid=model_params, scoring=make_scorer(scorer_func),\n",
    "                              n_jobs=-1, cv=model_dataset['cv'])\n",
    "            gs.fit(model_dataset['X_united'], model_dataset['y_united'])\n",
    "            y_pred = gs.best_estimator_.predict(model_dataset['X_test'])\n",
    "            test_score = scorer_func(model_dataset['y_test'], y_pred)\n",
    "            res.append({\"model\": model_name,\n",
    "                        \"scorer\": scorer_name,\n",
    "                        \"best_params\": gs.best_params_,\n",
    "                        \"val_score\": gs.best_score_,\n",
    "                        \"test_score\": test_score})\n",
    "            f.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>scorer</th>\n",
       "      <th>best_params</th>\n",
       "      <th>val_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf</td>\n",
       "      <td>roc-auc</td>\n",
       "      <td>{'class_weight': None, 'n_estimators': 20}</td>\n",
       "      <td>0.749635</td>\n",
       "      <td>0.754891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xgb</td>\n",
       "      <td>roc-auc</td>\n",
       "      <td>{'gamma': 0.3, 'max_depth': 10, 'n_estimators': 20}</td>\n",
       "      <td>0.763846</td>\n",
       "      <td>0.747267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logit</td>\n",
       "      <td>roc-auc</td>\n",
       "      <td>{'C': 0.1, 'class_weight': None, 'penalty': 'l1'}</td>\n",
       "      <td>0.692383</td>\n",
       "      <td>0.699223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svm</td>\n",
       "      <td>roc-auc</td>\n",
       "      <td>{'C': 0.1, 'class_weight': None}</td>\n",
       "      <td>0.635131</td>\n",
       "      <td>0.634781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rf</td>\n",
       "      <td>f1</td>\n",
       "      <td>{'class_weight': None, 'n_estimators': 20}</td>\n",
       "      <td>0.700565</td>\n",
       "      <td>0.682927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xgb</td>\n",
       "      <td>f1</td>\n",
       "      <td>{'gamma': 0.3, 'max_depth': 10, 'n_estimators': 20}</td>\n",
       "      <td>0.732673</td>\n",
       "      <td>0.674419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>logit</td>\n",
       "      <td>f1</td>\n",
       "      <td>{'C': 1, 'class_weight': 'balanced', 'penalty': 'l1'}</td>\n",
       "      <td>0.663551</td>\n",
       "      <td>0.547486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>svm</td>\n",
       "      <td>f1</td>\n",
       "      <td>{'C': 0.1, 'class_weight': 'balanced'}</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model   scorer                                            best_params  \\\n",
       "3  rf     roc-auc  {'class_weight': None, 'n_estimators': 20}              \n",
       "0  xgb    roc-auc  {'gamma': 0.3, 'max_depth': 10, 'n_estimators': 20}     \n",
       "1  logit  roc-auc  {'C': 0.1, 'class_weight': None, 'penalty': 'l1'}       \n",
       "2  svm    roc-auc  {'C': 0.1, 'class_weight': None}                        \n",
       "7  rf     f1       {'class_weight': None, 'n_estimators': 20}              \n",
       "4  xgb    f1       {'gamma': 0.3, 'max_depth': 10, 'n_estimators': 20}     \n",
       "5  logit  f1       {'C': 1, 'class_weight': 'balanced', 'penalty': 'l1'}   \n",
       "6  svm    f1       {'C': 0.1, 'class_weight': 'balanced'}                  \n",
       "\n",
       "   val_score  test_score  \n",
       "3  0.749635   0.754891    \n",
       "0  0.763846   0.747267    \n",
       "1  0.692383   0.699223    \n",
       "2  0.635131   0.634781    \n",
       "7  0.700565   0.682927    \n",
       "4  0.732673   0.674419    \n",
       "5  0.663551   0.547486    \n",
       "6  0.600000   0.523256    "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "tuned = pd.DataFrame(res, columns=[\"model\", \"scorer\", \"best_params\", \"val_score\", \"test_score\"])\n",
    "tuned.sort_values([\"scorer\", \"test_score\"], ascending=False, inplace=True)\n",
    "\n",
    "tuned.to_csv('data/results3.csv', index=False)\n",
    "tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning of hyperparameters we managed to achieve better results with all of the tested models (which is an obvious result)\n",
    "\n",
    "RF is still ahead of the other models. It seems pretty interesting that the best RF model has only 20 trees. I expected to see 2000 trees since authors of the paper used 2000 trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
