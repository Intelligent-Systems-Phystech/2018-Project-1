{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание Basic code.\n",
    "## По мотивам статьи: 2014 - On the Importance of Text Analysis for Stock Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load modules\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack, vstack\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 of 9 | ticker: AAPL\n",
      "iteration 2 of 9 | ticker: ADBE\n",
      "iteration 3 of 9 | ticker: AMZN\n",
      "iteration 4 of 9 | ticker: GOOG\n",
      "iteration 5 of 9 | ticker: HPQ\n",
      "iteration 6 of 9 | ticker: IBM\n",
      "iteration 7 of 9 | ticker: INTC\n",
      "iteration 8 of 9 | ticker: MSFT\n",
      "iteration 9 of 9 | ticker: NVDA\n",
      "finished. time elapsed: 92.32 sec\n"
     ]
    }
   ],
   "source": [
    "time_ = time.time()\n",
    "\n",
    "def create_reports(tickers):\n",
    "    sp500_ticker = \"gspc\" # S&P500 ticker from paper\n",
    "\n",
    "    reports_dataset = pd.DataFrame(columns=['ticker', 'date', 'time', 'text', 'movement', 'movement_normalized', 'label'])\n",
    "\n",
    "    for iter_, ticker in enumerate(tickers):\n",
    "        ############################################\n",
    "        ################## PART 1 ##################\n",
    "        ############################################\n",
    "\n",
    "        print('iteration %i of %i | ticker: %s' % (iter_+1, len(tickers), ticker))\n",
    "\n",
    "        # load data\n",
    "\n",
    "        # 1. stock quotes\n",
    "        price = pd.read_csv('data/price_history/'+ticker+'.csv')\n",
    "        price = price[price['Date'] > '2001-12-31']\n",
    "        price = price.sort_values('Date').reset_index(drop=True)\n",
    "        sp500 = pd.read_csv('data/price_history/'+sp500_ticker+'.csv')\n",
    "        sp500 = sp500[sp500['Date'] > '2001-12-31']\n",
    "        sp500 = sp500.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "        # 2. 8K reports\n",
    "        with open('data/8K/'+ticker, 'r') as f:\n",
    "            f_lines = f.readlines()\n",
    "        raw_8k_reports = pd.Series(' '.join(f_lines).split('</DOCUMENT>')[:-1])\n",
    "\n",
    "        def transform_8k_report_to_dataframe(report):\n",
    "            # transform 8k report to pd.DataFrame row\n",
    "            #\n",
    "            # report: string\n",
    "            # result: pd.DataFrame\n",
    "\n",
    "            result = pd.DataFrame(columns=['ticker', 'date', 'time', 'text'], index=[0])\n",
    "\n",
    "            result['ticker'] = report.split('FILE:')[1].split('/')[0]\n",
    "\n",
    "            datetime_ = report.split('TIME:')[1].split('\\n')[0]\n",
    "            datetime_ = datetime.datetime.strptime(datetime_, '%Y%m%d%H%M%S')\n",
    "            result['date'] = str(datetime_.date())\n",
    "            result['time'] = str(datetime_.time())\n",
    "\n",
    "            text = report.split('ITEM:')[-1]\n",
    "            text = text.replace('QuickLinks', '').replace('Click here to rapidly navigate through this document', '')\n",
    "            text = ' '.join(text.split())\n",
    "            result['text'] = text\n",
    "\n",
    "            return result\n",
    "\n",
    "        reports = pd.DataFrame(columns=[])\n",
    "        for report in raw_8k_reports:\n",
    "            row = transform_8k_report_to_dataframe(report)\n",
    "            reports = pd.concat([reports, row], axis=0)\n",
    "        reports.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        del f_lines, raw_8k_reports, report\n",
    "\n",
    "\n",
    "        ############################################\n",
    "        ################## PART 2 ##################\n",
    "        ############################################\n",
    "\n",
    "        # create binary markup {MOVE, STAY} for price data\n",
    "        # aggregate Up and DOWN labels to MOVE label\n",
    "\n",
    "        price_movement = pd.DataFrame(columns=['date', 'movement', 'movement_normalized', 'label'])\n",
    "\n",
    "        for i, j in price.iloc[:-1,:].iterrows():\n",
    "            row = pd.DataFrame(columns=['date', 'movement', 'movement_normalized', 'label'], index=[0])\n",
    "            row['date'] = j['Date']\n",
    "\n",
    "            column_next =  \"Open\"\n",
    "            column_prev =  \"Close\"\n",
    "            price_change = (price.loc[i+1, column_next] - price.loc[i, column_prev]) / price.loc[i, column_prev]\n",
    "            sp500_change = (sp500.loc[i+1, column_next] - sp500.loc[i, column_prev]) / sp500.loc[i, column_prev]\n",
    "            row['movement'] = price_change\n",
    "\n",
    "            price_change_normalized =  price_change - sp500_change\n",
    "            row['movement_normalized'] = price_change_normalized\n",
    "\n",
    "            if price_change_normalized >= 0.01: # value from paper\n",
    "                row['label'] = \"MOVE\" # price movement: UP\n",
    "            elif price_change_normalized <= -0.01: # value from paper\n",
    "                row['label'] =  \"MOVE\"# price movement: DOWN\n",
    "            else:\n",
    "                row['label'] =  \"STAY\"# price movement: STAY\n",
    "\n",
    "            price_movement = pd.concat([price_movement, row], axis=0)\n",
    "\n",
    "        price_movement.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        del price, sp500\n",
    "\n",
    "        # merge stock quotes and text\n",
    "\n",
    "        reports = pd.merge(reports, price_movement, on='date', how='left')\n",
    "        reports.dropna(axis=0, inplace=True)\n",
    "\n",
    "        del price_movement\n",
    "\n",
    "        # combine tickers\n",
    "\n",
    "        reports_dataset = pd.concat([reports_dataset, reports], axis=0)\n",
    "\n",
    "        del reports\n",
    "\n",
    "    reports_dataset.reset_index(drop=True, inplace=True)\n",
    "    return reports_dataset\n",
    "\n",
    "tickers = [\"AAPL\", \"ADBE\", \"AMZN\", \"GOOG\", \"HPQ\", \"IBM\", \"INTC\", \"MSFT\", \"NVDA\"] # tickers for {Apple,Adobe,Amazon,Google,HP,IBM,Intel,MicroSoft,NVidia}\n",
    "reports_dataset = create_reports(tickers)\n",
    "\n",
    "print('finished. time elapsed: %.2f sec' % (time.time() - time_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reports_dataset.to_csv(\"ReportsDatasetStep2.csv\")\n",
    "reports_dataset = pd.read_csv(\"ReportsDatasetStep2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "################## PART 3 ##################\n",
    "############################################\n",
    "\n",
    "# split data to train / validation / test\n",
    "\n",
    "# features\n",
    "def train_val_test_split(reports_dataset):\n",
    "    train_start, train_end = '2001-12-31', '2008-12-31' # train period from paper\n",
    "    x_train = reports_dataset[(train_start < reports_dataset['date']) & (reports_dataset['date'] <= train_end)]['text']\n",
    "\n",
    "    val_start, val_end = train_end, '2010-12-31'# validation period from paper\n",
    "    x_val = reports_dataset[(val_start < reports_dataset['date']) & (reports_dataset['date'] <= val_end)]['text']\n",
    "\n",
    "    test_start, test_end =  val_end, '2012-12-31'# test period from paper\n",
    "    x_test = reports_dataset[(test_start < reports_dataset['date']) & (reports_dataset['date'] <= test_end)]['text']\n",
    "\n",
    "    # target variable\n",
    "    # I added a comparison to \"MOVE\" in order to work with binary data explicitly\n",
    "    y_train = reports_dataset[(train_start < reports_dataset['date']) & (reports_dataset['date'] <= train_end)]['label'] == \"MOVE\"\n",
    "    y_val = reports_dataset[(val_start < reports_dataset['date']) & (reports_dataset['date'] <= val_end)]['label'] == \"MOVE\"\n",
    "    y_test = reports_dataset[(test_start < reports_dataset['date']) & (reports_dataset['date'] <= test_end)]['label'] == \"MOVE\"\n",
    "\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = train_val_test_split(reports_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2411 features using unigrams\n",
      "nmf 50 features created\n",
      "nmf 100 features created\n",
      "nmf 200 features created\n",
      "finished. time elapsed: 118.37 sec\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "################## PART 4 ##################\n",
    "############################################\n",
    "\n",
    "time_ = time.time()\n",
    "\n",
    "# create features\n",
    "\n",
    "def create_features(x_train, y_train, x_val, y_val, x_test, y_test):\n",
    "\n",
    "    # 1. Unigrams\n",
    "    # example: vectorizer_params = {}\n",
    "    #          vectorizer = Vectorizer(**vectorizer_params)\n",
    "    #          unigrams_train_features = vectorizer.fit_transform(x_train)\n",
    "    # ...\n",
    "    vectorizer_params = {\"ngram_range\": (1, 1), # we are using unigrams\n",
    "                         \"min_df\": 10} # according to authors\n",
    "\n",
    "    # I do not remove stop-words since it was not mentioned in the original paper\n",
    "\n",
    "    vectorizer = CountVectorizer(**vectorizer_params) \n",
    "    unigrams_train_features = vectorizer.fit_transform(x_train)\n",
    "\n",
    "    # I decided to try to implement feature selection using PMI\n",
    "    # Found the implementation guide here: \n",
    "    # https://stackoverflow.com/questions/46752650/information-gain-calculation-with-scikit-learn?rq=1\n",
    "\n",
    "    feature_scores = mutual_info_classif(unigrams_train_features, y_train, \n",
    "                                         discrete_features=True,\n",
    "                                         random_state=RANDOM_STATE)\n",
    "\n",
    "    vocab = [el for el, score in\n",
    "             zip(vectorizer.get_feature_names(), feature_scores) if\n",
    "             score > 0.01]\n",
    "\n",
    "    # Using 0.01 as a threshold since we ends up with 2411 features\n",
    "    # which is pretty close to 2319 features left in the original paper \n",
    "\n",
    "    vectorizer_params['vocabulary'] = vocab\n",
    "\n",
    "    vectorizer = CountVectorizer(**vectorizer_params) \n",
    "\n",
    "    unigrams_train_features = vectorizer.fit_transform(x_train)\n",
    "    unigrams_val_features = vectorizer.transform(x_val)\n",
    "    unigrams_test_features = vectorizer.transform(x_test)\n",
    "\n",
    "    print('Created {} features using unigrams'.format(unigrams_train_features.shape[1]))\n",
    "\n",
    "    # 2. NMF vector for 50, 100, and 200 components\n",
    "    # example: nmf_params = {}\n",
    "    #          nmf = NMF(**nmf_params)\n",
    "    #          nmf_train_features = nmf.fit_transform(unigrams_train_features)\n",
    "    # ...\n",
    "\n",
    "    nmf_params = {'n_components': 50,\n",
    "                  'random_state': RANDOM_STATE,\n",
    "                  'l1_ratio': 0.5,\n",
    "                  'alpha': 0.1}\n",
    "    # nmf_params were chosen based on\n",
    "    # https://scikit-learn.org/0.18/auto_examples/applications/topics_extraction_with_nmf_lda.html\n",
    "\n",
    "    nmf = NMF(**nmf_params)\n",
    "    nmf_50_train_features = nmf.fit_transform(unigrams_train_features)\n",
    "    nmf_50_val_features = nmf.transform(unigrams_val_features)\n",
    "    nmf_50_test_features = nmf.transform(unigrams_test_features)\n",
    "\n",
    "    print('nmf 50 features created')\n",
    "\n",
    "    nmf_params['n_components'] = 100\n",
    "    nmf = NMF(**nmf_params)\n",
    "    nmf_100_train_features = nmf.fit_transform(unigrams_train_features)\n",
    "    nmf_100_val_features = nmf.transform(unigrams_val_features)\n",
    "    nmf_100_test_features = nmf.transform(unigrams_test_features)\n",
    "\n",
    "    print('nmf 100 features created')\n",
    "\n",
    "    nmf_params['n_components'] = 200\n",
    "    nmf = NMF(**nmf_params)\n",
    "    nmf_200_train_features = nmf.fit_transform(unigrams_train_features)\n",
    "    nmf_200_val_features = nmf.transform(unigrams_val_features)\n",
    "    nmf_200_test_features = nmf.transform(unigrams_test_features)\n",
    "\n",
    "    print('nmf 200 features created')\n",
    "    return (unigrams_train_features, unigrams_val_features, unigrams_test_features,\n",
    "            nmf_50_train_features, nmf_50_val_features, nmf_50_test_features,\n",
    "            nmf_100_train_features, nmf_100_val_features, nmf_100_test_features,\n",
    "            nmf_200_train_features, nmf_200_val_features, nmf_200_test_features,\n",
    "            y_train, y_val, y_test)\n",
    "\n",
    "unigrams_train_features, unigrams_val_features, unigrams_test_features, \\\n",
    "    nmf_50_train_features, nmf_50_val_features, nmf_50_test_features, \\\n",
    "    nmf_100_train_features, nmf_100_val_features, nmf_100_test_features, \\\n",
    "    nmf_200_train_features, nmf_200_val_features, nmf_200_test_features, \\\n",
    "    y_train, y_val, y_test = \\\n",
    "    create_features(x_train, y_train, x_val, y_val, x_test, y_test)\n",
    "    \n",
    "print('finished. time elapsed: %.2f sec' % (time.time() - time_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "################## PART 5 ##################\n",
    "############################################\n",
    "\n",
    "# combine all features to one feature space\n",
    "\n",
    "# 1. NMF 50 features\n",
    "def unite_features(unigrams_train_features, unigrams_val_features, unigrams_test_features,\n",
    "                   nmf_50_train_features, nmf_50_val_features, nmf_50_test_features,\n",
    "                   nmf_100_train_features, nmf_100_val_features, nmf_100_test_features,\n",
    "                   nmf_200_train_features, nmf_200_val_features, nmf_200_test_features,\n",
    "                   y_train, y_val, y_test,\n",
    "                   dict_view=False):\n",
    "    nmf_50_x_train = hstack([unigrams_train_features, \n",
    "                             nmf_50_train_features])\n",
    "\n",
    "    nmf_50_x_val = hstack([unigrams_val_features,   \n",
    "                           nmf_50_val_features])\n",
    "\n",
    "    nmf_50_x_test = hstack([unigrams_test_features,  \n",
    "                            nmf_50_test_features])\n",
    "\n",
    "    # 2. NMF 100 features\n",
    "    nmf_100_x_train = hstack([unigrams_train_features, \n",
    "                             nmf_100_train_features])\n",
    "\n",
    "    nmf_100_x_val = hstack([unigrams_val_features,   \n",
    "                           nmf_100_val_features])\n",
    "\n",
    "    nmf_100_x_test = hstack([unigrams_test_features,  \n",
    "                            nmf_100_test_features])\n",
    "\n",
    "\n",
    "    # 3. NMF 200 features\n",
    "    nmf_200_x_train = hstack([unigrams_train_features, \n",
    "                             nmf_200_train_features])\n",
    "\n",
    "    nmf_200_x_val = hstack([unigrams_val_features,   \n",
    "                           nmf_200_val_features])\n",
    "\n",
    "    nmf_200_x_test = hstack([unigrams_test_features,  \n",
    "                            nmf_200_test_features])\n",
    "\n",
    "    # 4. Ensemble features\n",
    "    ensemble_x_train = hstack([unigrams_train_features, \n",
    "                               nmf_50_train_features, \n",
    "                               nmf_100_train_features, \n",
    "                               nmf_200_train_features])\n",
    "\n",
    "    ensemble_x_val = hstack([unigrams_val_features, \n",
    "                             nmf_50_val_features, \n",
    "                             nmf_100_val_features, \n",
    "                             nmf_200_val_features])\n",
    "\n",
    "    ensemble_x_test = hstack([unigrams_test_features, \n",
    "                              nmf_50_test_features, \n",
    "                              nmf_100_test_features, \n",
    "                              nmf_200_test_features])\n",
    "    data = (unigrams_train_features, unigrams_val_features,\n",
    "            unigrams_test_features,\n",
    "            nmf_50_x_train, nmf_50_x_val, nmf_50_x_test,\n",
    "            nmf_100_x_train, nmf_100_x_val, nmf_100_x_test,\n",
    "            nmf_200_x_train, nmf_200_x_val, nmf_200_x_test,\n",
    "            ensemble_x_train, ensemble_x_val, ensemble_x_test,\n",
    "            y_train, y_val, y_test)\n",
    "    if dict_view:\n",
    "        names = (\"unigrams_train_features\", \"unigrams_val_features\",\n",
    "                 \"unigrams_test_features\",\n",
    "                 \"nmf_50_x_train\", \"nmf_50_x_val\", \"nmf_50_x_test\",\n",
    "                 \"nmf_100_x_train\", \"nmf_100_x_val\", \"nmf_100_x_test\",\n",
    "                 \"nmf_200_x_train\", \"nmf_200_x_val\", \"nmf_200_x_test\",\n",
    "                 \"ensemble_x_train\", \"ensemble_x_val\", \"ensemble_x_test\",\n",
    "                 \"y_train\", \"y_val\", \"y_test\")\n",
    "        return dict(zip(names, data))\n",
    "    return data\n",
    "\n",
    "unigrams_train_features, unigrams_val_features, \\\n",
    "    unigrams_test_features, \\\n",
    "    nmf_50_x_train, nmf_50_x_val, nmf_50_x_test, \\\n",
    "    nmf_100_x_train, nmf_100_x_val, nmf_100_x_test, \\\n",
    "    nmf_200_x_train, nmf_200_x_val, nmf_200_x_test, \\\n",
    "    ensemble_x_train, ensemble_x_val, ensemble_x_test, \\\n",
    "    y_train, y_val, y_test = \\\n",
    "    unite_features(unigrams_train_features, unigrams_val_features, unigrams_test_features,\n",
    "                   nmf_50_train_features, nmf_50_val_features, nmf_50_test_features,\n",
    "                   nmf_100_train_features, nmf_100_val_features, nmf_100_test_features,\n",
    "                   nmf_200_train_features, nmf_200_val_features, nmf_200_test_features,\n",
    "                   y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_unigrams_model trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_nmf_50_model trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_nmf_100_model trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_nmf_200_model trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_ensemble_model trained\n",
      "finished. time elapsed: 30.44 sec\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "################## PART 6 ##################\n",
    "############################################\n",
    "\n",
    "time_ = time.time()\n",
    "\n",
    "# set basic classifier parameters, fit classifiers. \n",
    "# Hint: initialize new basic classifier before new training process\n",
    "\n",
    "# example: basic_classifier_params = {}\n",
    "#          basic_classifier = BasicClassifier(**basic_classifier_params)\n",
    "\n",
    "\n",
    "# 1. Unigrams\n",
    "# example: rf_unigrams_model = ...\n",
    "# ...\n",
    "rf_params = {\"n_estimators\": 2000, # 2000 trees according to the paper\n",
    "             \"n_jobs\": -1,\n",
    "             \"random_state\": RANDOM_STATE}\n",
    "rf_unigrams_model = RandomForestClassifier(**rf_params)\n",
    "rf_unigrams_model.fit(unigrams_train_features, y_train)\n",
    "\n",
    "print('rf_unigrams_model trained')\n",
    "\n",
    "# 2. NMF 50\n",
    "\n",
    "rf_nmf_50_model = RandomForestClassifier(**rf_params)\n",
    "rf_nmf_50_model.fit(nmf_50_x_train, y_train)\n",
    "\n",
    "print('rf_nmf_50_model trained')\n",
    "\n",
    "# 3. NMF 100\n",
    "rf_nmf_100_model = RandomForestClassifier(**rf_params)\n",
    "rf_nmf_100_model.fit(nmf_100_x_train, y_train)\n",
    "\n",
    "print('rf_nmf_100_model trained')\n",
    "\n",
    "# 4. NMF 200\n",
    "rf_nmf_200_model = RandomForestClassifier(**rf_params)\n",
    "rf_nmf_200_model.fit(nmf_200_x_train, y_train)\n",
    "\n",
    "print('rf_nmf_200_model trained')\n",
    "\n",
    "# 5. Ensemble\n",
    "rf_ensemble_model = RandomForestClassifier(**rf_params)\n",
    "rf_ensemble_model.fit(ensemble_x_train, y_train)\n",
    "\n",
    "print('rf_ensemble_model trained')\n",
    "print('finished. time elapsed: %.2f sec' % (time.time() - time_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>n_features</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rf_unigrams_model</td>\n",
       "      <td>2411</td>\n",
       "      <td>0.760163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf_nmf_50_model</td>\n",
       "      <td>2461</td>\n",
       "      <td>0.764228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rf_nmf_100_model</td>\n",
       "      <td>2511</td>\n",
       "      <td>0.768293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf_nmf_200_model</td>\n",
       "      <td>2611</td>\n",
       "      <td>0.764228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf_ensemble_model</td>\n",
       "      <td>2761</td>\n",
       "      <td>0.760163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model  n_features  accuracy\n",
       "0  rf_unigrams_model        2411  0.760163\n",
       "1    rf_nmf_50_model        2461  0.764228\n",
       "2   rf_nmf_100_model        2511  0.768293\n",
       "3   rf_nmf_200_model        2611  0.764228\n",
       "4  rf_ensemble_model        2761  0.760163"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################\n",
    "################## PART 7 ##################\n",
    "############################################\n",
    "\n",
    "# calculate quality measures\n",
    "\n",
    "# 1. Unigrams\n",
    "# example: rf_unigrams_quality = Quality(y_true, y_pred)\n",
    "\n",
    "y_pred = rf_unigrams_model.predict(unigrams_test_features)\n",
    "rf_unigrams_quality = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# 2. NMF 50\n",
    "y_pred = rf_nmf_50_model.predict(nmf_50_x_test)\n",
    "rf_nmf_50_quality = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# 3. NMF 100\n",
    "y_pred = rf_nmf_100_model.predict(nmf_100_x_test)\n",
    "rf_nmf_100_quality = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# 4. NMF 200\n",
    "y_pred = rf_nmf_200_model.predict(nmf_200_x_test)\n",
    "rf_nmf_200_quality = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# 5. Ensemble\n",
    "y_pred = rf_ensemble_model.predict(ensemble_x_test)\n",
    "rf_ensemble_quality = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# save results\n",
    "\n",
    "results = pd.DataFrame({'model': ['rf_unigrams_model', 'rf_nmf_50_model', \n",
    "                                  'rf_nmf_100_model', 'rf_nmf_200_model', 'rf_ensemble_model'], \n",
    "                        'n_features': [rf_unigrams_model.n_features_, rf_nmf_50_model.n_features_, \n",
    "                                       rf_nmf_100_model.n_features_, rf_nmf_200_model.n_features_, \n",
    "                                       rf_ensemble_model.n_features_], \n",
    "                        'accuracy': [rf_unigrams_quality, rf_nmf_50_quality, \n",
    "                                     rf_nmf_100_quality, rf_nmf_200_quality, rf_ensemble_quality]}, \n",
    "                       columns=['model', 'n_features', 'accuracy'])\n",
    "results.to_csv('data/results.csv', index=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I have less features since I'm using PMI for feature selection as it was mentioned in the original paper. I've decided to leave calculations with less features since it works faster and shows a better result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect where our best model makes errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[131,  27],\n",
       "       [ 30,  58]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rf_nmf_100_model.predict(nmf_100_x_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be pretty hard to analize false pozitive errors. Let's try to take a closer look to the example of false positive error (model believes that the piece of news will hane an effect on the share price)\n",
    "\n",
    "*Just a reminder: I set \"MOVE\" answer as True and \"STAY\" as False*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Submission of Matters to a Vote of Security Holders Check the appropriate box below if the Form 8-K filing is intended to simultaneously satisfy the filing obligation of the registrant under any of the following provisions: Item 5.07 Submission of Matters to a Vote of Security Holders The Annual Meeting of Shareholders (the \"Annual Meeting\") of Apple Inc. (the \"Company\") was held on February 23, 2011. At the Annual Meeting, the shareholders voted on the following six proposals and cast their votes as described below. Proposal 1 The individuals listed below were elected at the Annual Meeting to serve a one-year term on the Company\\'s Board of Directors (the \"Board\"). Proposal 2 Proposal 2 was a management proposal to ratify the appointment of Ernst & Young LLP as the Company\\'s independent registered public accounting firm for fiscal year 2011, as described in the proxy materials. This proposal was approved. Proposal 3 Proposal 3 was a management proposal to hold an advisory vote on executive compensation, as described in the proxy materials. This proposal was approved. Proposal 4 Proposal 4 was a management proposal to hold an advisory vote on the frequency of the advisory vote on executive compensation, as described in the proxy materials. \"1 Year\" was approved. Based on these results, and consistent with the Company\\'s recommendation, the Board has determined that the Company will hold an advisory vote on executive compensation every year. Proposal 5 Proposal 5 was a shareholder proposal entitled \"Amend the Company\\'s Corporate Governance Guidelines to Adopt and Disclose a Written CEO Succession Planning Policy,\" as described in the proxy materials. This proposal was not approved. Proposal 6 Proposal 6 was a shareholder proposal entitled \"Adopt a Majority Voting Standard for Director Elections,\" as described in the proxy materials. This proposal was approved. Pursuant to the requirements of the Securities Exchange Act of 1934, the Registrant has duly caused this report to be signed on its behalf by the undersigned, thereunto duly authorized.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positive = ((y_test == False) & (y_pred == True))\n",
    "x_test[false_positive.index[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We may see here some positive words like \"satisfy\", \"proposals\", \"approved\", \"recommendation\", \"consistent\". Yet, we, as a human beings may understand that it is not so positive information: this is all mainly about some company governance tasks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating new datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I'm going to create 2 new datasets using the algorithm used in the previous section.\n",
    "\n",
    "The first one, banks_dataset will be formed based on the information from the 3 large banks: JPMorgan Chase & Co. (JPM), Bank of America Corp (BAC) and Citigroup Inc. (C)\n",
    "\n",
    "The second one, cg_dataset will be formed based on the information from the 3 large companies which provides consumer goods: Gap Inc. (GPS), Nike (NKE) and Hasbro Inc. (HAS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 of 3 | ticker: JPM\n",
      "iteration 2 of 3 | ticker: BAC\n",
      "iteration 3 of 3 | ticker: C\n",
      "Created 1129 features using unigrams\n",
      "nmf 50 features created\n",
      "nmf 100 features created\n",
      "nmf 200 features created\n",
      "finished. time elapsed: 140.28 sec\n"
     ]
    }
   ],
   "source": [
    "time_ = time.time()\n",
    "banks_tickers = [\"JPM\", \"BAC\", \"C\"]\n",
    "banks_dataset = unite_features(*create_features(*train_val_test_split(create_reports(banks_tickers))),\n",
    "                               dict_view=True)\n",
    "print('finished. time elapsed: %.2f sec' % (time.time() - time_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 of 3 | ticker: GPS\n",
      "iteration 2 of 3 | ticker: NKE\n",
      "iteration 3 of 3 | ticker: HAS\n",
      "Created 1961 features using unigrams\n",
      "nmf 50 features created\n",
      "nmf 100 features created\n",
      "nmf 200 features created\n",
      "finished. time elapsed: 96.46 sec\n"
     ]
    }
   ],
   "source": [
    "time_ = time.time()\n",
    "cg_tickers = [\"GPS\", \"NKE\", \"HAS\"]\n",
    "cg_dataset = unite_features(*create_features(*train_val_test_split(create_reports(cg_tickers))),\n",
    "                            dict_view=True)\n",
    "print('finished. time elapsed: %.2f sec' % (time.time() - time_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, name):\n",
    "    prepped = {}\n",
    "    prepped[\"name\"] = name\n",
    "    X = {}\n",
    "    X[\"unigrams\"] = {\"train\": dataset['unigrams_train_features'],\n",
    "                     \"val\": dataset['unigrams_val_features'],\n",
    "                     \"test\": dataset['unigrams_test_features']}   \n",
    "    X[\"nmf50\"] = {\"train\": dataset['nmf_50_x_train'],\n",
    "                  \"val\": dataset['nmf_50_x_val'],\n",
    "                  \"test\": dataset['nmf_50_x_test']}\n",
    "    X[\"nmf100\"] = {\"train\": dataset['nmf_100_x_train'],\n",
    "                   \"val\": dataset['nmf_100_x_val'],\n",
    "                   \"test\": dataset['nmf_100_x_test']}\n",
    "    X[\"nmf200\"] = {\"train\": dataset['nmf_200_x_train'],\n",
    "                   \"val\": dataset['nmf_200_x_val'],\n",
    "                   \"test\": dataset['nmf_200_x_test']}\n",
    "    X[\"ensemble\"] = {\"train\": dataset['ensemble_x_train'],\n",
    "                     \"val\": dataset['ensemble_x_val'],\n",
    "                     \"test\": dataset['ensemble_x_test']}\n",
    "    prepped[\"X\"] = X\n",
    "    prepped['y'] = {\"train\": dataset['y_train'],\n",
    "                    \"val\": dataset['y_val'],\n",
    "                    \"test\": dataset['y_test']}\n",
    "    return prepped\n",
    "    \n",
    "banks_dataset = prepare_dataset(banks_dataset, \"banks\")\n",
    "cg_dataset = prepare_dataset(cg_dataset, \"consumer_goods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to build some models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section I'm going to check the performance of XGBClassifier, LinearSVC and LogisticRegression on the dataset created in the previous section using f1-score and roc-auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use not tuned classifiers in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams trained\n",
      "nmf50 trained\n",
      "nmf100 trained\n",
      "nmf200 trained\n",
      "ensemble trained\n",
      "unigrams trained\n",
      "nmf50 trained\n",
      "nmf100 trained\n",
      "nmf200 trained\n",
      "ensemble trained\n",
      "finished. time elapsed: 15.09 sec\n"
     ]
    }
   ],
   "source": [
    "time_ = time.time()\n",
    "\n",
    "params = {\"random_state\": RANDOM_STATE}\n",
    "\n",
    "def create_and_fit_models(dataset, params):\n",
    "    trained_models = {}\n",
    "    for features_names, X_data in dataset['X'].items():\n",
    "        X = X_data[\"train\"]\n",
    "        y = dataset['y'][\"train\"]\n",
    "        trained_models[features_names] = \\\n",
    "            {\"xgb\": XGBClassifier(n_jobs=-1, **params).fit(X, y),\n",
    "             \"logit\": LogisticRegression(**params).fit(X, y),\n",
    "             \"svm\": LinearSVC(**params).fit(X, y),\n",
    "             \"rf\": RandomForestClassifier(**params).fit(X, y)}\n",
    "        print('{} trained'.format(features_names))\n",
    "    return trained_models\n",
    "\n",
    "\n",
    "banks_models = create_and_fit_models(banks_dataset, params)\n",
    "cg_models = create_and_fit_models(cg_dataset, params)\n",
    "\n",
    "print('finished. time elapsed: %.2f sec' % (time.time() - time_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_quality_scores(dataset, models, test_plan):\n",
    "    res = []\n",
    "    for features_names, X_data in dataset['X'].items():\n",
    "        X_test = X_data['test']\n",
    "        y_test = dataset['y']['test']        \n",
    "        for model_name, model in models[features_names].items():\n",
    "            y_pred = model.predict(X_test)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            roc_auc = roc_auc_score(y_test, y_pred)\n",
    "            res.append({\"model\": model_name,\n",
    "                        \"dataset\": dataset[\"name\"],\n",
    "                        \"features\": features_names,\n",
    "                        \"f1 score\": f1,\n",
    "                        \"roc_auc_score\": roc_auc})\n",
    "    return res\n",
    "\n",
    "banks_summary = get_quality_scores(banks_dataset, banks_models, testing_plan)\n",
    "cg_summary = get_quality_scores(cg_dataset, cg_models, testing_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(banks_summary + cg_summary, columns=[\"dataset\", \"model\", \"features\", \"f1 score\", \"roc_auc_score\"])\n",
    "df.sort_values([\"dataset\", \"model\", \"f1 score\"], ascending=False, inplace=True)\n",
    "\n",
    "df.to_csv('data/results2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>features</th>\n",
       "      <th>f1 score</th>\n",
       "      <th>roc_auc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>xgb</td>\n",
       "      <td>nmf100</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.526786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>xgb</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.549107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>xgb</td>\n",
       "      <td>nmf200</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.551339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>xgb</td>\n",
       "      <td>unigrams</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>xgb</td>\n",
       "      <td>nmf50</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.426339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>svm</td>\n",
       "      <td>nmf200</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>svm</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>svm</td>\n",
       "      <td>unigrams</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.540179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>svm</td>\n",
       "      <td>nmf50</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.540179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>svm</td>\n",
       "      <td>nmf100</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.540179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>rf</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>0.495536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>rf</td>\n",
       "      <td>unigrams</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.526786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>rf</td>\n",
       "      <td>nmf50</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.517857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>rf</td>\n",
       "      <td>nmf100</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.506696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>rf</td>\n",
       "      <td>nmf200</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.462054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>logit</td>\n",
       "      <td>nmf200</td>\n",
       "      <td>0.412698</td>\n",
       "      <td>0.542411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>logit</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>0.412698</td>\n",
       "      <td>0.542411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>logit</td>\n",
       "      <td>nmf50</td>\n",
       "      <td>0.393443</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>logit</td>\n",
       "      <td>unigrams</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.526786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>logit</td>\n",
       "      <td>nmf100</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.526786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>banks</td>\n",
       "      <td>xgb</td>\n",
       "      <td>nmf200</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.471996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>banks</td>\n",
       "      <td>xgb</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>0.084507</td>\n",
       "      <td>0.470867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>banks</td>\n",
       "      <td>xgb</td>\n",
       "      <td>unigrams</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.458672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>banks</td>\n",
       "      <td>xgb</td>\n",
       "      <td>nmf50</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.458672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>banks</td>\n",
       "      <td>xgb</td>\n",
       "      <td>nmf100</td>\n",
       "      <td>0.054795</td>\n",
       "      <td>0.449413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>banks</td>\n",
       "      <td>svm</td>\n",
       "      <td>unigrams</td>\n",
       "      <td>0.405797</td>\n",
       "      <td>0.531617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>banks</td>\n",
       "      <td>svm</td>\n",
       "      <td>nmf50</td>\n",
       "      <td>0.405797</td>\n",
       "      <td>0.531617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>banks</td>\n",
       "      <td>svm</td>\n",
       "      <td>nmf200</td>\n",
       "      <td>0.390977</td>\n",
       "      <td>0.525294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>banks</td>\n",
       "      <td>svm</td>\n",
       "      <td>nmf100</td>\n",
       "      <td>0.388060</td>\n",
       "      <td>0.521229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>banks</td>\n",
       "      <td>svm</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>0.388060</td>\n",
       "      <td>0.521229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>banks</td>\n",
       "      <td>rf</td>\n",
       "      <td>nmf100</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.501581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>banks</td>\n",
       "      <td>rf</td>\n",
       "      <td>unigrams</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>0.477191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>banks</td>\n",
       "      <td>rf</td>\n",
       "      <td>nmf200</td>\n",
       "      <td>0.126582</td>\n",
       "      <td>0.464995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>banks</td>\n",
       "      <td>rf</td>\n",
       "      <td>nmf50</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.483062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>banks</td>\n",
       "      <td>rf</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>0.059701</td>\n",
       "      <td>0.473803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>banks</td>\n",
       "      <td>logit</td>\n",
       "      <td>ensemble</td>\n",
       "      <td>0.393443</td>\n",
       "      <td>0.543360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>banks</td>\n",
       "      <td>logit</td>\n",
       "      <td>nmf200</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>0.531165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>banks</td>\n",
       "      <td>logit</td>\n",
       "      <td>nmf50</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.523713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>banks</td>\n",
       "      <td>logit</td>\n",
       "      <td>nmf100</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0.523261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>banks</td>\n",
       "      <td>logit</td>\n",
       "      <td>unigrams</td>\n",
       "      <td>0.241758</td>\n",
       "      <td>0.496161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dataset  model  features  f1 score  roc_auc_score\n",
       "28  consumer_goods    xgb    nmf100  0.313725       0.526786\n",
       "36  consumer_goods    xgb  ensemble  0.279070       0.549107\n",
       "32  consumer_goods    xgb    nmf200  0.250000       0.551339\n",
       "20  consumer_goods    xgb  unigrams  0.205128       0.535714\n",
       "24  consumer_goods    xgb     nmf50  0.046512       0.426339\n",
       "34  consumer_goods    svm    nmf200  0.470588       0.571429\n",
       "38  consumer_goods    svm  ensemble  0.470588       0.571429\n",
       "22  consumer_goods    svm  unigrams  0.424242       0.540179\n",
       "26  consumer_goods    svm     nmf50  0.424242       0.540179\n",
       "30  consumer_goods    svm    nmf100  0.424242       0.540179\n",
       "39  consumer_goods     rf  ensemble  0.244898       0.495536\n",
       "23  consumer_goods     rf  unigrams  0.200000       0.526786\n",
       "27  consumer_goods     rf     nmf50  0.195122       0.517857\n",
       "31  consumer_goods     rf    nmf100  0.058824       0.506696\n",
       "35  consumer_goods     rf    nmf200  0.051282       0.462054\n",
       "33  consumer_goods  logit    nmf200  0.412698       0.542411\n",
       "37  consumer_goods  logit  ensemble  0.412698       0.542411\n",
       "25  consumer_goods  logit     nmf50  0.393443       0.535714\n",
       "21  consumer_goods  logit  unigrams  0.387097       0.526786\n",
       "29  consumer_goods  logit    nmf100  0.387097       0.526786\n",
       "12           banks    xgb    nmf200  0.108108       0.471996\n",
       "16           banks    xgb  ensemble  0.084507       0.470867\n",
       "0            banks    xgb  unigrams  0.081081       0.458672\n",
       "4            banks    xgb     nmf50  0.081081       0.458672\n",
       "8            banks    xgb    nmf100  0.054795       0.449413\n",
       "2            banks    svm  unigrams  0.405797       0.531617\n",
       "6            banks    svm     nmf50  0.405797       0.531617\n",
       "14           banks    svm    nmf200  0.390977       0.525294\n",
       "10           banks    svm    nmf100  0.388060       0.521229\n",
       "18           banks    svm  ensemble  0.388060       0.521229\n",
       "11           banks     rf    nmf100  0.142857       0.501581\n",
       "3            banks     rf  unigrams  0.131579       0.477191\n",
       "15           banks     rf    nmf200  0.126582       0.464995\n",
       "7            banks     rf     nmf50  0.088235       0.483062\n",
       "19           banks     rf  ensemble  0.059701       0.473803\n",
       "17           banks  logit  ensemble  0.393443       0.543360\n",
       "13           banks  logit    nmf200  0.384000       0.531165\n",
       "5            banks  logit     nmf50  0.358974       0.523713\n",
       "9            banks  logit    nmf100  0.326923       0.523261\n",
       "1            banks  logit  unigrams  0.241758       0.496161"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building not tuned models for banks and consumer goods companies we may notice that the quality of the classification is higher when we use linear models. Probably this result may be explained by the size of the feature space: linear models always show pretty good performance when we use many features, especially if a lot of them have not any effect on the resulting variable\n",
    "\n",
    "It is likely that we may see the increase of quality of ensemble models if we tune them a little. Let's try to do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we may use our validation dataset to tune parameters of our models!\n",
    "\n",
    "We are going to tune models using those features which allowed to achieve the best quality with each specific model.\n",
    "\n",
    "We will use f1 score and roc-auc score to compare our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_validation(dataset):\n",
    "    \"\"\"\n",
    "    we want to tune parameters using validation set\n",
    "    GridSearchCV with default cv param wouldn't help us with that\n",
    "    We need to use PredefinedSplit as mentioned in\n",
    "    https://stackoverflow.com/questions/43764999/python-machine-learning-perform-a-grid-search-on-custom-validation-set/43766334#43766334\n",
    "    \"\"\"\n",
    "    my_test_fold = []\n",
    "    for features_names, X_data in dataset['X'].items(): \n",
    "        X_data['united'] = sparse.vstack([X_data['train'], X_data['val']])\n",
    "    \n",
    "    for i in range(X_data['train'].shape[0]):\n",
    "        my_test_fold.append(-1)\n",
    "\n",
    "    for i in range(X_data['val'].shape[0]):\n",
    "        my_test_fold.append(0)\n",
    "\n",
    "    dataset['y']['united'] = np.concatenate([dataset['y']['train'].values, dataset['y']['val'].values])\n",
    "    dataset['split'] = PredefinedSplit(test_fold=my_test_fold)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "banks_dataset = prep_validation(banks_dataset)\n",
    "cg_dataset = prep_validation(cg_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_to_dataset = {\n",
    "    (\"banks\", \"xgb\"): \"nmf200\",\n",
    "    (\"banks\", \"logit\"): \"ensemble\",\n",
    "    (\"banks\", \"svm\"): \"unigrams\",\n",
    "    (\"banks\", \"rf\"): \"nmf100\",\n",
    "    (\"consumer_goods\", \"xgb\"): \"nmf100\",\n",
    "    (\"consumer_goods\", \"logit\"): \"nmf200\",\n",
    "    (\"consumer_goods\", \"svm\"): \"nmf200\",\n",
    "    (\"consumer_goods\", \"rf\"): \"ensemble\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_model_params = {\n",
    "    \"xgb\": {\"gamma\": [0.25, 0.5],\n",
    "            \"max_depth\": [3, 6, 10],\n",
    "            \"n_estimators\": [20, 200]},\n",
    "    \"logit\": {\"penalty\": [\"l1\", \"l2\"],\n",
    "              \"C\": [0.1, 1, 10, 100]},\n",
    "    \"svm\": {\"class_weight\": [None, \"balanced\"],\n",
    "            \"C\": [0.1, 1, 10, 100]},\n",
    "    \"rf\": {\"n_estimators\": [10, 20, 200, 2000]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95fad24a4621421c9693c1daa354133d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic',\n",
       "       random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'gamma': [0.0, 0.3, 0.5], 'max_depth': [3, 6, 10], 'n_estimators': [20, 200]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(roc_auc_score), verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'class_weight': [None, 'balanced'], 'C': [0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(roc_auc_score), verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
       "     verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'class_weight': [None, 'balanced'], 'C': [0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(roc_auc_score), verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'class_weight': [None, 'balanced'], 'n_estimators': [10, 20, 200, 2000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(roc_auc_score), verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic',\n",
       "       random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'gamma': [0.0, 0.3, 0.5], 'max_depth': [3, 6, 10], 'n_estimators': [20, 200]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(roc_auc_score), verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'class_weight': [None, 'balanced'], 'C': [0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(roc_auc_score), verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
       "     verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'class_weight': [None, 'balanced'], 'C': [0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(roc_auc_score), verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'class_weight': [None, 'balanced'], 'n_estimators': [10, 20, 200, 2000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(roc_auc_score), verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic',\n",
       "       random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'gamma': [0.0, 0.3, 0.5], 'max_depth': [3, 6, 10], 'n_estimators': [20, 200]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(f1_score), verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'class_weight': [None, 'balanced'], 'C': [0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(f1_score), verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
       "     verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'class_weight': [None, 'balanced'], 'C': [0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(f1_score), verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'class_weight': [None, 'balanced'], 'n_estimators': [10, 20, 200, 2000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(f1_score), verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic',\n",
       "       random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'gamma': [0.0, 0.3, 0.5], 'max_depth': [3, 6, 10], 'n_estimators': [20, 200]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(f1_score), verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'class_weight': [None, 'balanced'], 'C': [0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(f1_score), verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
       "     verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'class_weight': [None, 'balanced'], 'C': [0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(f1_score), verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'class_weight': [None, 'balanced'], 'n_estimators': [10, 20, 200, 2000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(f1_score), verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "time_ = time.time()\n",
    "res = []\n",
    "with tqdm_notebook(total=2*2*4) as f:\n",
    "    for scorer_func, scorer_name in [(roc_auc_score, \"roc-auc\"), (f1_score, \"f1\")]:\n",
    "        for dataset in [banks_dataset, cg_dataset]:\n",
    "            models = {\"xgb\": XGBClassifier(random_state=RANDOM_STATE),\n",
    "                      \"logit\": LogisticRegression(random_state=RANDOM_STATE),\n",
    "                      \"svm\": LinearSVC(random_state=RANDOM_STATE),\n",
    "                      \"rf\": RandomForestClassifier(random_state=RANDOM_STATE)}\n",
    "            for model_name, model in models.items():\n",
    "                model_X = dataset[\"X\"][model_to_dataset[(dataset[\"name\"], model_name)]]\n",
    "                model_params = all_model_params[model_name]\n",
    "                gs = GridSearchCV(model, param_grid=model_params, scoring=make_scorer(scorer_func),\n",
    "                                  n_jobs=-1, cv=dataset['split'])\n",
    "                gs.fit(model_X['united'], dataset['y']['united']);\n",
    "                y_pred = gs.best_estimator_.predict(model_X['test'])\n",
    "                test_score = scorer_func(dataset['y']['test'], y_pred)\n",
    "                res.append({\"dataset\": dataset['name'],\n",
    "                            \"model\": model_name,\n",
    "                            \"scorer\": scorer_name,\n",
    "                            \"best_params\": gs.best_params_,\n",
    "                            \"val_score\": gs.best_score_,\n",
    "                            \"test_score\": test_score})\n",
    "                f.update()\n",
    "\n",
    "print('finished. time elapsed: %.2f sec' % (time.time() - time_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>scorer</th>\n",
       "      <th>best_params</th>\n",
       "      <th>val_score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>rf</td>\n",
       "      <td>roc-auc</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 200}</td>\n",
       "      <td>0.689189</td>\n",
       "      <td>0.667411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>xgb</td>\n",
       "      <td>roc-auc</td>\n",
       "      <td>{'gamma': 0.0, 'max_depth': 10, 'n_estimators': 20}</td>\n",
       "      <td>0.666216</td>\n",
       "      <td>0.645089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>logit</td>\n",
       "      <td>roc-auc</td>\n",
       "      <td>{'C': 0.1, 'class_weight': None, 'penalty': 'l1'}</td>\n",
       "      <td>0.680270</td>\n",
       "      <td>0.584821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>svm</td>\n",
       "      <td>roc-auc</td>\n",
       "      <td>{'C': 0.1, 'class_weight': None}</td>\n",
       "      <td>0.564324</td>\n",
       "      <td>0.524554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>rf</td>\n",
       "      <td>f1</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 200}</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.530612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>xgb</td>\n",
       "      <td>f1</td>\n",
       "      <td>{'gamma': 0.0, 'max_depth': 10, 'n_estimators': 20}</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>svm</td>\n",
       "      <td>f1</td>\n",
       "      <td>{'C': 0.1, 'class_weight': None}</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>consumer_goods</td>\n",
       "      <td>logit</td>\n",
       "      <td>f1</td>\n",
       "      <td>{'C': 100, 'class_weight': None, 'penalty': 'l1'}</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.354839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>banks</td>\n",
       "      <td>svm</td>\n",
       "      <td>roc-auc</td>\n",
       "      <td>{'C': 100, 'class_weight': None}</td>\n",
       "      <td>0.602273</td>\n",
       "      <td>0.584914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>banks</td>\n",
       "      <td>logit</td>\n",
       "      <td>roc-auc</td>\n",
       "      <td>{'C': 100, 'class_weight': None, 'penalty': 'l2'}</td>\n",
       "      <td>0.519318</td>\n",
       "      <td>0.506098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>banks</td>\n",
       "      <td>rf</td>\n",
       "      <td>roc-auc</td>\n",
       "      <td>{'class_weight': 'balanced', 'n_estimators': 2000}</td>\n",
       "      <td>0.530682</td>\n",
       "      <td>0.505194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>banks</td>\n",
       "      <td>xgb</td>\n",
       "      <td>roc-auc</td>\n",
       "      <td>{'gamma': 0.3, 'max_depth': 10, 'n_estimators': 200}</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.454607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>banks</td>\n",
       "      <td>svm</td>\n",
       "      <td>f1</td>\n",
       "      <td>{'C': 0.1, 'class_weight': 'balanced'}</td>\n",
       "      <td>0.598214</td>\n",
       "      <td>0.449612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>banks</td>\n",
       "      <td>logit</td>\n",
       "      <td>f1</td>\n",
       "      <td>{'C': 1, 'class_weight': 'balanced', 'penalty': 'l2'}</td>\n",
       "      <td>0.504854</td>\n",
       "      <td>0.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>banks</td>\n",
       "      <td>xgb</td>\n",
       "      <td>f1</td>\n",
       "      <td>{'gamma': 0.3, 'max_depth': 10, 'n_estimators': 200}</td>\n",
       "      <td>0.427481</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>banks</td>\n",
       "      <td>rf</td>\n",
       "      <td>f1</td>\n",
       "      <td>{'class_weight': None, 'n_estimators': 200}</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dataset  model   scorer  \\\n",
       "7   consumer_goods  rf     roc-auc   \n",
       "4   consumer_goods  xgb    roc-auc   \n",
       "5   consumer_goods  logit  roc-auc   \n",
       "6   consumer_goods  svm    roc-auc   \n",
       "15  consumer_goods  rf     f1        \n",
       "12  consumer_goods  xgb    f1        \n",
       "14  consumer_goods  svm    f1        \n",
       "13  consumer_goods  logit  f1        \n",
       "2   banks           svm    roc-auc   \n",
       "1   banks           logit  roc-auc   \n",
       "3   banks           rf     roc-auc   \n",
       "0   banks           xgb    roc-auc   \n",
       "10  banks           svm    f1        \n",
       "9   banks           logit  f1        \n",
       "8   banks           xgb    f1        \n",
       "11  banks           rf     f1        \n",
       "\n",
       "                                              best_params  val_score  \\\n",
       "7   {'class_weight': 'balanced', 'n_estimators': 200}      0.689189    \n",
       "4   {'gamma': 0.0, 'max_depth': 10, 'n_estimators': 20}    0.666216    \n",
       "5   {'C': 0.1, 'class_weight': None, 'penalty': 'l1'}      0.680270    \n",
       "6   {'C': 0.1, 'class_weight': None}                       0.564324    \n",
       "15  {'class_weight': 'balanced', 'n_estimators': 200}      0.549020    \n",
       "12  {'gamma': 0.0, 'max_depth': 10, 'n_estimators': 20}    0.551724    \n",
       "14  {'C': 0.1, 'class_weight': None}                       0.551724    \n",
       "13  {'C': 100, 'class_weight': None, 'penalty': 'l1'}      0.615385    \n",
       "2   {'C': 100, 'class_weight': None}                       0.602273    \n",
       "1   {'C': 100, 'class_weight': None, 'penalty': 'l2'}      0.519318    \n",
       "3   {'class_weight': 'balanced', 'n_estimators': 2000}     0.530682    \n",
       "0   {'gamma': 0.3, 'max_depth': 10, 'n_estimators': 200}   0.590909    \n",
       "10  {'C': 0.1, 'class_weight': 'balanced'}                 0.598214    \n",
       "9   {'C': 1, 'class_weight': 'balanced', 'penalty': 'l2'}  0.504854    \n",
       "8   {'gamma': 0.3, 'max_depth': 10, 'n_estimators': 200}   0.427481    \n",
       "11  {'class_weight': None, 'n_estimators': 200}            0.260870    \n",
       "\n",
       "    test_score  \n",
       "7   0.667411    \n",
       "4   0.645089    \n",
       "5   0.584821    \n",
       "6   0.524554    \n",
       "15  0.530612    \n",
       "12  0.526316    \n",
       "14  0.400000    \n",
       "13  0.354839    \n",
       "2   0.584914    \n",
       "1   0.506098    \n",
       "3   0.505194    \n",
       "0   0.454607    \n",
       "10  0.449612    \n",
       "9   0.390625    \n",
       "8   0.080000    \n",
       "11  0.031250    "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "tuned = pd.DataFrame(res, columns=[\"dataset\", \"model\", \"scorer\", \"best_params\", \"val_score\", \"test_score\"])\n",
    "tuned.sort_values([\"dataset\", \"scorer\", \"test_score\"], ascending=False, inplace=True)\n",
    "\n",
    "tuned.to_csv('data/results3.csv', index=False)\n",
    "tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning of hyperparameters we see that tuned RF and XGB models outperforms tuned linear models on the dataset of consumer goods companies. And we may notice 180 degrees situation with banks dataset. Also we may notice that there is a higher f1 and roc-auc scores for consumer goods companies.\n",
    "\n",
    "Probably, it may be explained by the excessive amount of financial data in banks reports and lack of the changes in the banking sector (this means that the information on the banks 8K reports do not influence on the core business and as a result we have a lot of the useless features): that is why in banking sector we see a lower quality of classificators and the supremacy of linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this notebook we tried to build a model inspired by the paper \"On the Importance of Text Analysis for Stock Price Prediction\". We created different set of features based on the vectorization of the 8K reports text. We managed to prove that there is a connection between 8K reports and stock proces behavior.**\n",
    "\n",
    "**After that we builded 4 different models using data of 3 major American banks and 3 major consumer goods companies. We noticed that not tuned linear models shows a better quality on this data than not tuned ensemble models**\n",
    "\n",
    "**Finally, we tried to tune those models using features which gave us the best quality on the previous step and made some very interesting conclusions using this results**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
