{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание Basic code.\n",
    "## По мотивам статьи: 2014 - [On the Importance of Text Analysis for Stock Price Prediction](https://nlp.stanford.edu/pubs/lrec2014-stock.pdf)\n",
    "## [Данные](https://nlp.stanford.edu/pubs/stock-event.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# load modules\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack, vstack\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "from pyunpack import Archive\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  accuracy_score # quality measure from paper\n",
    "\n",
    "from sklearn.ensemble import  RandomForestClassifier # basic classifier from paper\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "    print('Скачай данные из https://nlp.stanford.edu/pubs/stock-event.html в папку data!')\n",
    "    \n",
    "if not os.path.exists('data\\my8K'):\n",
    "    os.mkdir('data\\my8K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 of 9 | ticker: AAPL\n",
      "iteration 2 of 9 | ticker: ADBE\n",
      "iteration 3 of 9 | ticker: AMZN\n",
      "iteration 4 of 9 | ticker: GOOG\n",
      "iteration 5 of 9 | ticker: HPQ\n",
      "iteration 6 of 9 | ticker: IBM\n",
      "iteration 7 of 9 | ticker: INTC\n",
      "iteration 8 of 9 | ticker: MSFT\n",
      "iteration 9 of 9 | ticker: NVDA\n",
      "finished. time elapsed: 118.94 sec\n"
     ]
    }
   ],
   "source": [
    "time_ = time.time()\n",
    "\n",
    "tickers = ['AAPL', 'ADBE', 'AMZN', 'GOOG', 'HPQ', 'IBM', 'INTC', 'MSFT', 'NVDA'] # tickers for {Apple,Adobe,Amazon,Google,HP,IBM,Intel,MicroSoft,NVidia}\n",
    "sp500_ticker =  'gspc'# S&P500 ticker from paper\n",
    "\n",
    "price_history_path = 'data\\price_history\\\\'\n",
    "eightk_path = 'data\\8K\\\\'\n",
    "\n",
    "reports_dataset = pd.DataFrame(columns=['ticker', 'date', 'time', 'text', 'movement', 'movement_normalized', 'label'])\n",
    "\n",
    "for iter_, ticker in enumerate(tickers):\n",
    "    ############################################\n",
    "    ################## PART 1 ##################\n",
    "    ############################################\n",
    "    \n",
    "    print('iteration %i of %i | ticker: %s' % (iter_+1, len(tickers), ticker))\n",
    "    \n",
    "    # load data\n",
    "    \n",
    "    # 1. stock quotes\n",
    "    price = pd.read_csv(price_history_path + ticker+'.csv')\n",
    "    price = price[price['Date'] > '2001-12-31']\n",
    "    price = price.sort_values('Date').reset_index(drop=True)\n",
    "    sp500 = pd.read_csv(price_history_path+sp500_ticker+'.csv')\n",
    "    sp500 = sp500[sp500['Date'] > '2001-12-31']\n",
    "    sp500 = sp500.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    # 2. 8K reports\n",
    "    try:\n",
    "        Archive('data\\8K\\\\' + ticker + '.gz').extractall('data\\my8K')\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "    with open('data\\my8K\\\\' + ticker, 'r') as f:\n",
    "        f_lines = f.readlines()\n",
    "    raw_8k_reports = pd.Series(' '.join(f_lines).split('</DOCUMENT>')[:-1])\n",
    "\n",
    "    def transform_8k_report_to_dataframe(report):\n",
    "        # transform 8k report to pd.DataFrame row\n",
    "        #\n",
    "        # report: string\n",
    "        # result: pd.DataFrame\n",
    "\n",
    "        result = pd.DataFrame(columns=['ticker', 'date', 'time', 'text'], index=[0])\n",
    "\n",
    "        result['ticker'] = report.split('FILE:')[1].split('/')[0]\n",
    "\n",
    "        datetime_ = report.split('TIME:')[1].split('\\n')[0]\n",
    "        datetime_ = datetime.datetime.strptime(datetime_, '%Y%m%d%H%M%S')\n",
    "        result['date'] = str(datetime_.date())\n",
    "        result['time'] = str(datetime_.time())\n",
    "\n",
    "        text = report.split('ITEM:')[-1]\n",
    "        text = text.replace('QuickLinks', '').replace('Click here to rapidly navigate through this document', '')\n",
    "        text = ' '.join(text.split())\n",
    "        result['text'] = text\n",
    "\n",
    "        return result\n",
    "\n",
    "    reports = pd.DataFrame(columns=[])\n",
    "    for report in raw_8k_reports:\n",
    "        row = transform_8k_report_to_dataframe(report)\n",
    "        reports = pd.concat([reports, row], axis=0)\n",
    "    reports.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    del f_lines, raw_8k_reports, report\n",
    "    \n",
    "    \n",
    "    ############################################\n",
    "    ################## PART 2 ##################\n",
    "    ############################################\n",
    "    \n",
    "    # create binary markup {MOVE, STAY} for price data\n",
    "    # aggregate Up and DOWN labels to MOVE label\n",
    "\n",
    "    price_movement = pd.DataFrame(columns=['date', 'movement', 'movement_normalized', 'label'])\n",
    "\n",
    "    for i, j in price.iloc[:-1,:].iterrows():\n",
    "        row = pd.DataFrame(columns=['date', 'movement', 'movement_normalized', 'label'], index=[0])\n",
    "        row['date'] = j['Date']\n",
    "\n",
    "        column_next =  'Open' # value from paper\n",
    "        column_prev =  'Close' # value from paper\n",
    "        price_change = (price.loc[i+1, column_next] - price.loc[i, column_prev]) / price.loc[i, column_prev]\n",
    "        sp500_change = (sp500.loc[i+1, column_next] - sp500.loc[i, column_prev]) / sp500.loc[i, column_prev]\n",
    "        row['movement'] = price_change\n",
    "\n",
    "        price_change_normalized =  price_change - sp500_change# price_change normalization, described in paper\n",
    "        row['movement_normalized'] = price_change_normalized\n",
    "\n",
    "        if price_change_normalized >= 0.01: # value from paper\n",
    "            row['label'] =  'UP' # price movement: UP\n",
    "        elif price_change_normalized <= -0.01: # value from paper\n",
    "            row['label'] =  'DOWN' # price movement: DOWN\n",
    "        else:\n",
    "            row['label'] =  'STAY' # price movement: STAY\n",
    "\n",
    "        price_movement = pd.concat([price_movement, row], axis=0)\n",
    "\n",
    "    price_movement.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    del price, sp500\n",
    "    \n",
    "    # merge stock quotes and text\n",
    "\n",
    "    reports = pd.merge(reports, price_movement, on='date', how='left')\n",
    "    reports.dropna(axis=0, inplace=True)\n",
    "\n",
    "    del price_movement\n",
    "    \n",
    "    # combine tickers\n",
    "    \n",
    "    reports_dataset = pd.concat([reports_dataset, reports], axis=0)\n",
    "    \n",
    "    del reports\n",
    "\n",
    "reports_dataset.reset_index(drop=True, inplace=True)\n",
    "print('finished. time elapsed: %.2f sec' % (time.time() - time_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "################## PART 3 ##################\n",
    "############################################\n",
    "\n",
    "# split data to train / validation / test\n",
    "\n",
    "# features\n",
    "train_start, train_end = '2001-12-31', '2008-12-31' # train period from paper\n",
    "x_train = reports_dataset[(train_start < reports_dataset['date']) & (reports_dataset['date'] <= train_end)]['text']\n",
    "\n",
    "val_start, val_end =  '2008-12-31', '2010-12-31' # validation period from paper\n",
    "x_val = reports_dataset[(val_start < reports_dataset['date']) & (reports_dataset['date'] <= val_end)]['text']\n",
    "\n",
    "test_start, test_end = '2010-12-31', '2012-12-31' # test period from paper\n",
    "x_test = reports_dataset[(test_start < reports_dataset['date']) & (reports_dataset['date'] <= test_end)]['text']\n",
    "\n",
    "# target variable\n",
    "y_train = reports_dataset[(train_start < reports_dataset['date']) & (reports_dataset['date'] <= train_end)]['label']\n",
    "y_val = reports_dataset[(val_start < reports_dataset['date']) & (reports_dataset['date'] <= val_end)]['label']\n",
    "y_test = reports_dataset[(test_start < reports_dataset['date']) & (reports_dataset['date'] <= test_end)]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams created\n",
      "nmf 50 features created\n",
      "nmf 100 features created\n",
      "nmf 200 features created\n",
      "finished. time elapsed: 386.83 sec\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "################## PART 4 ##################\n",
    "############################################\n",
    "\n",
    "time_ = time.time()\n",
    "\n",
    "# create features\n",
    "\n",
    "# 1. Unigrams\n",
    "# example: vectorizer_params = {}\n",
    "#          vectorizer = Vectorizer(**vectorizer_params)\n",
    "#          unigrams_train_features = vectorizer.fit_transform(x_train)\n",
    "# ...\n",
    "#vectorizer = CountVectorizer()\n",
    "vectorizer = TfidfVectorizer()\n",
    "unigrams_train_features = vectorizer.fit_transform(x_train)\n",
    "unigrams_val_features = vectorizer.transform(x_val)\n",
    "unigrams_test_features = vectorizer.transform(x_test)\n",
    "print('unigrams created')\n",
    "# To check feature names: vectorizer.get_feature_names()\n",
    "\n",
    "# 2. NMF vector for 50, 100, and 200 components\n",
    "# example: nmf_params = {}\n",
    "#          nmf = NMF(**nmf_params)\n",
    "#          nmf_train_features = nmf.fit_transform(unigrams_train_features)\n",
    "\n",
    "nmf50 = NMF(50)\n",
    "nmf_50_train_features = nmf50.fit_transform(unigrams_train_features)\n",
    "nmf_50_val_features = nmf50.transform(unigrams_val_features)\n",
    "nmf_50_test_features = nmf50.transform(unigrams_test_features)\n",
    "print('nmf 50 features created')\n",
    "# To check matrix shape: nmf_50_train_features.shape\n",
    "\n",
    "nmf100 = NMF(100)\n",
    "nmf_100_train_features = nmf100.fit_transform(unigrams_train_features)\n",
    "nmf_100_val_features = nmf100.transform(unigrams_val_features)\n",
    "nmf_100_test_features = nmf100.transform(unigrams_test_features)\n",
    "print('nmf 100 features created')\n",
    "\n",
    "nmf200 = NMF(200)\n",
    "nmf_200_train_features = nmf200.fit_transform(unigrams_train_features)\n",
    "nmf_200_val_features = nmf200.transform(unigrams_val_features)\n",
    "nmf_200_test_features = nmf200.transform(unigrams_test_features)\n",
    "print('nmf 200 features created')\n",
    "print('finished. time elapsed: %.2f sec' % (time.time() - time_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "################## PART 5 ##################\n",
    "############################################\n",
    "\n",
    "# combine all features to one feature space\n",
    "\n",
    "# 1. NMF 50 features\n",
    "nmf_50_x_train = hstack([unigrams_train_features, \n",
    "                         nmf_50_train_features])\n",
    "\n",
    "nmf_50_x_val = hstack([unigrams_val_features,   \n",
    "                       nmf_50_val_features])\n",
    "\n",
    "nmf_50_x_test = hstack([unigrams_test_features,  \n",
    "                        nmf_50_test_features])\n",
    "\n",
    "# 2. NMF 100 features\n",
    "nmf_100_x_train = hstack([unigrams_train_features, \n",
    "                         nmf_100_train_features])\n",
    "\n",
    "nmf_100_x_val = hstack([unigrams_val_features,   \n",
    "                       nmf_100_val_features])\n",
    "\n",
    "nmf_100_x_test = hstack([unigrams_test_features,  \n",
    "                        nmf_100_test_features])\n",
    "\n",
    "# 3. NMF 200 features\n",
    "nmf_200_x_train = hstack([unigrams_train_features, \n",
    "                         nmf_200_train_features])\n",
    "\n",
    "nmf_200_x_val = hstack([unigrams_val_features,   \n",
    "                       nmf_200_val_features])\n",
    "\n",
    "nmf_200_x_test = hstack([unigrams_test_features,  \n",
    "                        nmf_200_test_features])\n",
    "\n",
    "# 4. Ensemble features\n",
    "ensemble_x_train = hstack([unigrams_train_features, \n",
    "                           nmf_50_train_features, \n",
    "                           nmf_100_train_features, \n",
    "                           nmf_200_train_features])\n",
    "\n",
    "ensemble_x_val = hstack([unigrams_val_features, \n",
    "                           nmf_50_val_features, \n",
    "                           nmf_100_val_features, \n",
    "                           nmf_200_val_features])\n",
    "\n",
    "ensemble_x_test = hstack([unigrams_test_features, \n",
    "                           nmf_50_test_features, \n",
    "                           nmf_100_test_features, \n",
    "                           nmf_200_test_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_unigrams_model trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_nmf_50_model trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_nmf_100_model trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_nmf_200_model trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_ensemble_model trained\n",
      "finished. time elapsed: 140.82 sec\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "################## PART 6 ##################\n",
    "############################################\n",
    "\n",
    "time_ = time.time()\n",
    "\n",
    "# set basic classifier parameters, fit classifiers. \n",
    "# Hint: initialize new basic classifier before new training process\n",
    "\n",
    "# example: basic_classifier_params = {}\n",
    "#          basic_classifier = BasicClassifier(**basic_classifier_params)\n",
    "\n",
    "\n",
    "# 1. Unigrams\n",
    "# example: rf_unigrams_model = ...\n",
    "# ...\n",
    "#basic_classifier = BasicClassifier(**basic_classifier_params)\n",
    "#clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "#                             min_samples_split=2, random_state=0)\n",
    "rf_unigrams_model = RandomForestClassifier()\n",
    "rf_unigrams_model.fit(unigrams_train_features, y_train)\n",
    "#print(clf.feature_importances_)\n",
    "#scores = cross_val_score(clf, X, y, cv=5)\n",
    "#scores.mean()\n",
    "print('rf_unigrams_model trained')\n",
    "\n",
    "# 2. NMF 50\n",
    "rf_nmf_50_model = RandomForestClassifier(n_estimators=2000)\n",
    "rf_nmf_50_model.fit(nmf_50_x_train, y_train)\n",
    "\n",
    "print('rf_nmf_50_model trained')\n",
    "\n",
    "# 3. NMF 100\n",
    "rf_nmf_100_model = RandomForestClassifier(n_estimators=2000)\n",
    "rf_nmf_100_model.fit(nmf_100_x_train, y_train)\n",
    "\n",
    "print('rf_nmf_100_model trained')\n",
    "\n",
    "# 4. NMF 200\n",
    "rf_nmf_200_model = RandomForestClassifier(n_estimators=2000)\n",
    "rf_nmf_200_model.fit(nmf_200_x_train, y_train)\n",
    "\n",
    "print('rf_nmf_200_model trained')\n",
    "\n",
    "# 5. Ensemble\n",
    "rf_ensemble_model = RandomForestClassifier(n_estimators=2000)\n",
    "rf_ensemble_model.fit(ensemble_x_train, y_train)\n",
    "\n",
    "print('rf_ensemble_model trained')\n",
    "print('finished. time elapsed: %.2f sec' % (time.time() - time_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>n_features</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rf_unigrams_model</td>\n",
       "      <td>16420</td>\n",
       "      <td>0.586667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf_nmf_50_model</td>\n",
       "      <td>16470</td>\n",
       "      <td>0.662222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rf_nmf_100_model</td>\n",
       "      <td>16520</td>\n",
       "      <td>0.657778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf_nmf_200_model</td>\n",
       "      <td>16620</td>\n",
       "      <td>0.657778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf_ensemble_model</td>\n",
       "      <td>16770</td>\n",
       "      <td>0.662222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model  n_features  accuracy\n",
       "0  rf_unigrams_model       16420  0.586667\n",
       "1    rf_nmf_50_model       16470  0.662222\n",
       "2   rf_nmf_100_model       16520  0.657778\n",
       "3   rf_nmf_200_model       16620  0.657778\n",
       "4  rf_ensemble_model       16770  0.662222"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################\n",
    "################## PART 7 ##################\n",
    "############################################\n",
    "\n",
    "# calculate quality measures\n",
    "\n",
    "# 1. Unigrams\n",
    "# example: rf_unigrams_quality = Quality(y_true, y_pred)\n",
    "rf_unigrams_quality = accuracy_score(y_val, rf_unigrams_model.predict(unigrams_val_features))\n",
    "\n",
    "# 2. NMF 50\n",
    "rf_nmf_50_quality = accuracy_score(y_val, rf_nmf_50_model.predict(nmf_50_x_val))\n",
    "\n",
    "# 3. NMF 100\n",
    "rf_nmf_100_quality = accuracy_score(y_val, rf_nmf_100_model.predict(nmf_100_x_val))\n",
    "\n",
    "# 4. NMF 200\n",
    "rf_nmf_200_quality = accuracy_score(y_val, rf_nmf_200_model.predict(nmf_200_x_val))\n",
    "\n",
    "# 5. Ensemble\n",
    "rf_ensemble_quality = accuracy_score(y_val, rf_ensemble_model.predict(ensemble_x_val))\n",
    "\n",
    "# save results\n",
    "results = pd.DataFrame({'model': ['rf_unigrams_model', 'rf_nmf_50_model', \n",
    "                                  'rf_nmf_100_model', 'rf_nmf_200_model', 'rf_ensemble_model'], \n",
    "                        'n_features': [rf_unigrams_model.n_features_, rf_nmf_50_model.n_features_, \n",
    "                                       rf_nmf_100_model.n_features_, rf_nmf_200_model.n_features_, \n",
    "                                       rf_ensemble_model.n_features_], \n",
    "                        'accuracy': [rf_unigrams_quality, rf_nmf_50_quality, \n",
    "                                     rf_nmf_100_quality, rf_nmf_200_quality, rf_ensemble_quality]}, \n",
    "                       columns=['model', 'n_features', 'accuracy'])\n",
    "results.to_csv('data/results.csv', index=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
